{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m9WrRVLYlLra",
        "G_Y5diT0_CK5",
        "Ng0CFeAZyxl2",
        "1LTSh1vXy1fg",
        "eOFln2pmy4pt",
        "-rOFnV17y8Q-",
        "Sa6ydGidy_CI",
        "K90RnrEWB2Iw",
        "Jt5GdJ-uwNW8",
        "RsoiT-rUcsbi",
        "uILR-VuOtg1G",
        "wlwsWq26Gf5V",
        "T8zpvEiavYtE",
        "8GnVvI-7yNI_",
        "bUE7hxpjv0-O",
        "niiMUsYNy2Xy",
        "5-lJAmjGyx4w"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jsXWnc9abSrT"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 패키지 설치 & 라이브러리 import"
      ],
      "metadata": {
        "id": "m9WrRVLYlLra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lPMKbVWF2Gq",
        "outputId": "4974bb2d-8755-4f38-ab4f-631f50dcd985"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 colorlog-6.8.2 optuna-4.0.0\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "gKd7dfaTPzbm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전체 공통 함수"
      ],
      "metadata": {
        "id": "G_Y5diT0_CK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 데이터 보간 함수"
      ],
      "metadata": {
        "id": "Ng0CFeAZyxl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터의 시간 간격을 0.5초의 등간격으로 맞추고 그에 따른 좌표 값을 보간함\n",
        "- 가장 많이 사용되는 3차 스플라인 보간 적용"
      ],
      "metadata": {
        "id": "wk6vfDy4zsh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import interp1d\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 보간 - 위도, 고도, 경도 값을 0.5초 시간 간격으로 보간\n",
        "def spline_interpolation(x, y):\n",
        "    # x: 보간할 데이터의 x 값 (시간)\n",
        "    # y: 보간할 데이터의 y 값 (위도, 고도, 경도)\n",
        "\n",
        "    # 3차 스플라인 보간 함수 생성\n",
        "    f = interp1d(x, y, kind='cubic')\n",
        "\n",
        "    # 시간 범위 설정\n",
        "    start_time = min(x)\n",
        "    end_time = max(x)\n",
        "\n",
        "    # 0.5초 간격의 시간 배열을 생성\n",
        "    new_x = np.arange(start_time, end_time, 0.5)\n",
        "\n",
        "    # 위도, 경도, 고도 보간\n",
        "    new_y = f(new_x)\n",
        "\n",
        "    return new_x, new_y"
      ],
      "metadata": {
        "id": "MbOt2kIkQg_7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 시퀀스 나누는 함수"
      ],
      "metadata": {
        "id": "1LTSh1vXy1fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습을 위해 데이터를 시퀀스 형태로 나눔\n",
        "- look back과 forward에 따라 학습에 사용될 x값의 구간과 y값을 반환함\n",
        "- 초기 look back은 10, forward는 0이다."
      ],
      "metadata": {
        "id": "lWB9uZyCz1Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시퀀스 생성 함수\n",
        "# look back : 10 (5초간의 위도, 고도, 경도 데이터를 x값으로 사용)\n",
        "# forward : 0 (0.5초 뒤의 위도, 고도, 경도 데이터를 y값으로 사용)\n",
        "def create_sequences(df, look_back=10, forward=0):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for i in range(len(df) - look_back - forward):\n",
        "        x = df.iloc[i:(i+look_back)].values\n",
        "        y = df.iloc[i+look_back+forward].values\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)"
      ],
      "metadata": {
        "id": "_kmQ-_EyGFpW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 시간 소수점 절사 함수"
      ],
      "metadata": {
        "id": "eOFln2pmy4pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 불필요한 시간 소수점 제거"
      ],
      "metadata": {
        "id": "CDwOPPA05KGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# 시간 소수점 절사 함수\n",
        "def truncation(df):\n",
        "    new_time = []\n",
        "\n",
        "    for a in df['time']:\n",
        "      new_time.append(math.floor(a * 10) / 10)\n",
        "\n",
        "    df['time'] = new_time\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "CFiscMuBSycb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 스케일링 함수"
      ],
      "metadata": {
        "id": "-rOFnV17y8Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MinMaxScaling 적용 함수\n"
      ],
      "metadata": {
        "id": "442iH9qd_UIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# MinMaxScaling 적용 함수\n",
        "def min_max_scaling(df):\n",
        "\n",
        "    # time 삭제 df 생성\n",
        "    mid_df=df.drop(columns='time')\n",
        "\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.fit(mid_df)\n",
        "\n",
        "    min_max_data = min_max_scaler.transform(mid_df)\n",
        "\n",
        "    new_data = pd.DataFrame(min_max_data)\n",
        "\n",
        "    # time 추가\n",
        "    new_data.insert(0, 'time', df['time'])\n",
        "\n",
        "    # 나머지 열의 이름 가져오기\n",
        "    column_names = mid_df.columns.tolist()\n",
        "\n",
        "    column_names.insert(0, 'time')\n",
        "\n",
        "    print(column_names)\n",
        "\n",
        "    # 새로운 데이터프레임에 열 이름 설정\n",
        "    new_data.columns = column_names\n",
        "\n",
        "    return min_max_scaler, new_data"
      ],
      "metadata": {
        "id": "zR9B-lKYNgs8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. train/validation/test 분리 함수"
      ],
      "metadata": {
        "id": "Sa6ydGidy_CI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Train, Validation, Test 데이터셋을 분리하는 함수이다.\n",
        "- 각각의 비율은 6:2:2 이다."
      ],
      "metadata": {
        "id": "Ii6xjYd3A7YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train/validation/test 분리 함수\n",
        "def split_train_val_test(X, y):\n",
        "\n",
        "    #Train, Test 분류\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Train, Validation 분류\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "    # # 데이터 크기 출력\n",
        "    # print(\"Train set:\", X_train.shape, y_train.shape)\n",
        "    # print(\"Validation set:\", X_val.shape, y_val.shape)\n",
        "    # print(\"Test set:\", X_test.shape, y_test.shape)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "metadata": {
        "id": "mXJlnnQq7FgL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 경로 시각화 함수"
      ],
      "metadata": {
        "id": "K90RnrEWB2Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 실제 경로와 예측 경로를 각각 그래프로 생성\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t2bNHfpSBPfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 경로 시각화 함수\n",
        "def prediction_visualization(y_test, y_pred):\n",
        "    # 시각화를 위해 위도, 경도, 고도 분리\n",
        "    y_test_lat = y_test[:, 0]\n",
        "    y_test_lon = y_test[:, 1]\n",
        "    y_test_alt = y_test[:, 2]\n",
        "\n",
        "    y_pred_lat = y_pred[:, 0]\n",
        "    y_pred_lon = y_pred[:, 1]\n",
        "    y_pred_alt = y_pred[:, 2]\n",
        "\n",
        "    # 3D 그래프 생성\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # 실제 값 플롯\n",
        "    ax.scatter(y_test_lat, y_test_lon, y_test_alt, c='b', marker='o', label='Actual')\n",
        "\n",
        "    # 예측 값 플롯\n",
        "    ax.scatter(y_pred_lat, y_pred_lon, y_pred_alt, c='r', marker='^', label='Predicted')\n",
        "\n",
        "    # 그래프 제목 및 축 레이블 설정\n",
        "    ax.set_title('Actual vs Predicted')\n",
        "    ax.set_xlabel('Latitude')\n",
        "    ax.set_ylabel('Longitude')\n",
        "    ax.set_zlabel('Altitude')\n",
        "\n",
        "    # 범례\n",
        "    ax.legend()\n",
        "\n",
        "    # 그래프를 화면에 출력\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "l8bKkBe-h6cG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 데이터로더 생성 함수"
      ],
      "metadata": {
        "id": "Jt5GdJ-uwNW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습에 사용할 데이터 로더 생성\n",
        "1. 시퀀스 생성\n",
        "2. Train, Validation, Test 분리\n",
        "3. DataLoader 생성"
      ],
      "metadata": {
        "id": "__Dcj9VbwfKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로더 생성 함수\n",
        "def create_dataloaders(tri, batch_size, sequence_length):\n",
        "\n",
        "  tri = tri[['time', 'lat', 'lon', 'alt']]\n",
        "\n",
        "  # 시퀀스 생성\n",
        "  train_x, train_y = create_sequences(tri[['lat', 'lon', 'alt']], sequence_length)\n",
        "\n",
        "  # train, validation, test 분리\n",
        "  X_train, X_val, X_test, y_train, y_val, y_test = split_train_val_test(train_x, train_y)\n",
        "\n",
        "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "  y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "  X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "  y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "  y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "  train_dataset = TensorDataset(X_train, y_train)\n",
        "  validation_dataset = TensorDataset(X_val, y_val)\n",
        "  test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "  # 데이터 로더 생성\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "IfsHAblYwU0v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "RsoiT-rUcsbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 데이터셋 가져오기"
      ],
      "metadata": {
        "id": "4YJd91PCtHJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 데이터셋 가져오기 (필요시 저장 경로 수정)\n",
        "raw = pd.read_csv(\"/content/OnboardGPS.csv\")"
      ],
      "metadata": {
        "id": "MuunCpw0crPo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 출력\n",
        "# raw"
      ],
      "metadata": {
        "id": "yEjs_s1FfNBG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 데이터셋 전처리"
      ],
      "metadata": {
        "id": "uILR-VuOtg1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터셋 보간\n",
        "- 소수점 절사\n",
        "- MinMaxScaling 적용\n"
      ],
      "metadata": {
        "id": "ea6emhMrtjFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tri = raw[['Timpstemp', ' lat', ' lon', ' alt']]\n",
        "\n",
        "new_x, lat = spline_interpolation(tri['Timpstemp'] / 1e6 , tri[' lat'])\n",
        "new_x, lon = spline_interpolation(tri['Timpstemp'] / 1e6 , tri[' lon'])\n",
        "new_x, alt = spline_interpolation(tri['Timpstemp'] / 1e6 , tri[' alt'])\n",
        "\n",
        "tri = pd.DataFrame({\"time\":new_x, \"lat\":lat, \"lon\":lon, \"alt\":alt})\n",
        "\n",
        "tri = truncation(tri)\n",
        "scaler, tri = min_max_scaling(tri)\n",
        "# tri"
      ],
      "metadata": {
        "id": "0M4Mw46Ge5dr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836fe287-6c46-4af3-a31a-bfa71a71b544"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['time', 'lat', 'lon', 'alt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 선언"
      ],
      "metadata": {
        "id": "wlwsWq26Gf5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 모델 클래스 선언"
      ],
      "metadata": {
        "id": "XCmztXYWwA1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LSTM을 사용한 모델 클래스 선언\n",
        "- 구조 : LSTM + Fully Connected Layer"
      ],
      "metadata": {
        "id": "Pk72DAdQvxkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import optuna\n",
        "\n",
        "# LSTM 모델 클래스 정의\n",
        "class LSTM_Model(nn.Module):\n",
        "    def __init__(self, input_size=3, hidden_size=128, num_layers=2, output_size=3):\n",
        "        super(LSTM_Model, self).__init__()\n",
        "\n",
        "        # hidden state size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTM layer 수\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        # LSTM hidden state, cell state 초기화\n",
        "        # 초기 hidden state와 cell state 정의\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # LSTM 레이어를 통과\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # 마지막 시점의 hidden state에서 Fully Connected Layer를 통과\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "1-WDOIn-GiMc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 하이퍼파라미터 탐색"
      ],
      "metadata": {
        "id": "FYz0CNRn3w9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Optuna의 objective 함수 선언"
      ],
      "metadata": {
        "id": "T8zpvEiavYtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 최적의 하이퍼파라미터 탐색을 위해 optuna 사용\n",
        "- objective 함수 선언"
      ],
      "metadata": {
        "id": "7Tr7p_Llvd3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "\n",
        "def objective(trial, tri, sequence_length):\n",
        "    # 탐색할 하이퍼파라미터 값 결정\n",
        "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 5)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 16, 128)\n",
        "\n",
        "    num_epochs = 200\n",
        "    input_size = 3  # lat, lon, alt\n",
        "    output_size = 3  # lat, lon, alt\n",
        "\n",
        "    # 데이터셋 가져오기\n",
        "    train_loader, val_loader, _ = create_dataloaders(tri, batch_size, sequence_length)\n",
        "\n",
        "    # 모델 정의: LSTMModel\n",
        "    model = LSTM_Model(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    total_val_loss = 0.0  # 모든 epoch에서의 validation loss의 합\n",
        "    val_loss_count = 0    # 평균을 계산하기 위한 epoch 수\n",
        "\n",
        "    # 모델 학습\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Train set으로 학습\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation set에서 성능 측정\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                batch_y = batch_y.to(device)\n",
        "                output = model(batch_x)\n",
        "                loss = criterion(output, batch_y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # 배치 수로 나눠서 평균 validation 손실 계산\n",
        "        val_loss_avg = val_loss / len(val_loader)\n",
        "\n",
        "        # Optuna에 중간 값 보고\n",
        "        trial.report(val_loss_avg, epoch)\n",
        "\n",
        "        # 총 validation 손실 계산\n",
        "        total_val_loss += val_loss_avg\n",
        "        val_loss_count += 1\n",
        "\n",
        "        # Pruning 조건 확인\n",
        "        if trial.should_prune():\n",
        "            print(f\"Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            print(f\"Validation Loss at prune: {val_loss_avg:.4f}\")  # Prune 시점에서 validation loss 출력\n",
        "\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    # 모든 epoch에서의 validation 손실 평균을 반환\n",
        "    return total_val_loss / val_loss_count"
      ],
      "metadata": {
        "id": "ATdrPIwIgpHn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. early stopping 함수 선언"
      ],
      "metadata": {
        "id": "zbQgifbFjDnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- optuna에서 하이퍼파라미터 탐색 중 early stopping 적용"
      ],
      "metadata": {
        "id": "mAxKy5BBvnpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  # https://github.com/optuna/optuna/issues/1001\n",
        "\n",
        "# def early_stopping_check(study, trial, early_stopping_rounds=10):\n",
        "#   current_trial_number = trial.number\n",
        "#   best_trial_number = study.best_trial.number\n",
        "#   should_stop = (current_trial_number - best_trial_number) >= early_stopping_rounds\n",
        "#   if should_stop:\n",
        "#       print(f\"Early stopping detected: stopping trial {current_trial_number} (best trial {best_trial_number}) at epoch {epoch}\")\n",
        "#       study.stop()"
      ],
      "metadata": {
        "id": "Nzav0lS0jDAI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 탐색 범위 외의 하이퍼파라미터 선언"
      ],
      "metadata": {
        "id": "PfydFgp_qeMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 10\n",
        "early_stopping_rounds = 10\n",
        "n_trials = 30"
      ],
      "metadata": {
        "id": "FduoAs3iqfoW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 하이퍼 파라미터 탐색"
      ],
      "metadata": {
        "id": "-xyOYO-vyJxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from functools import partial\n",
        "\n",
        "# Optuna 스터디 생성 및 최적화 실행\n",
        "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=10))\n",
        "# study.optimize(lambda trial: objective(trial, tri, sequence_length), callbacks=[partial(early_stopping_check, early_stopping_rounds=early_stopping_rounds)], n_trials=n_trials)\n",
        "study.optimize(lambda trial: objective(trial, tri, sequence_length), n_trials=n_trials)\n",
        "\n",
        "# 최적의 하이퍼파라미터 출력\n",
        "print(\"Best hyperparameters: \", study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcflFUAU4q5p",
        "outputId": "fe4901be-36d1-4861-d7ea-5e0af61c3c23"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:24:55,388] A new study created in memory with name: no-name-a5f51c59-3982-40ca-9ee3-367da4d6da86\n",
            "[I 2024-09-29 10:25:41,499] Trial 0 finished with value: 0.0007332196273559275 and parameters: {'hidden_size': 125, 'num_layers': 3, 'learning_rate': 0.00030582884384162555, 'batch_size': 82}. Best is trial 0 with value: 0.0007332196273559275.\n",
            "[I 2024-09-29 10:27:04,389] Trial 1 finished with value: 0.00046976840351665804 and parameters: {'hidden_size': 134, 'num_layers': 5, 'learning_rate': 0.0010997966961354584, 'batch_size': 63}. Best is trial 1 with value: 0.00046976840351665804.\n",
            "[I 2024-09-29 10:27:30,767] Trial 2 finished with value: 0.0028589686492867085 and parameters: {'hidden_size': 92, 'num_layers': 3, 'learning_rate': 0.00012876671849975827, 'batch_size': 122}. Best is trial 1 with value: 0.00046976840351665804.\n",
            "[I 2024-09-29 10:29:53,425] Trial 3 finished with value: 0.00013862811564949884 and parameters: {'hidden_size': 100, 'num_layers': 5, 'learning_rate': 0.003905963865002013, 'batch_size': 22}. Best is trial 3 with value: 0.00013862811564949884.\n",
            "[I 2024-09-29 10:32:03,849] Trial 4 finished with value: 0.00018772848773314761 and parameters: {'hidden_size': 181, 'num_layers': 4, 'learning_rate': 0.0063986283807369515, 'batch_size': 44}. Best is trial 3 with value: 0.00013862811564949884.\n",
            "[I 2024-09-29 10:33:29,993] Trial 5 finished with value: 8.27182105760282e-05 and parameters: {'hidden_size': 159, 'num_layers': 3, 'learning_rate': 0.003975864068942424, 'batch_size': 37}. Best is trial 5 with value: 8.27182105760282e-05.\n",
            "[I 2024-09-29 10:34:29,901] Trial 6 finished with value: 7.63784429738242e-05 and parameters: {'hidden_size': 223, 'num_layers': 2, 'learning_rate': 0.007306449608861762, 'batch_size': 46}. Best is trial 6 with value: 7.63784429738242e-05.\n",
            "[I 2024-09-29 10:34:34,630] Trial 7 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:34:40,681] Trial 8 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:34:46,519] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:34:48,187] Trial 10 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:34:53,542] Trial 11 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 11 pruned at epoch 23\n",
            "Validation Loss at prune: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:35:48,170] Trial 12 finished with value: 6.99600516297364e-05 and parameters: {'hidden_size': 194, 'num_layers': 2, 'learning_rate': 0.0028279006922833096, 'batch_size': 51}. Best is trial 12 with value: 6.99600516297364e-05.\n",
            "[I 2024-09-29 10:35:53,432] Trial 13 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 13 pruned at epoch 16\n",
            "Validation Loss at prune: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:38:29,940] Trial 14 finished with value: 7.445146965240457e-05 and parameters: {'hidden_size': 197, 'num_layers': 2, 'learning_rate': 0.009962232134648667, 'batch_size': 16}. Best is trial 12 with value: 6.99600516297364e-05.\n",
            "[I 2024-09-29 10:38:40,363] Trial 15 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 pruned at epoch 14\n",
            "Validation Loss at prune: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:38:42,051] Trial 16 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 16 pruned at epoch 12\n",
            "Validation Loss at prune: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:38:45,205] Trial 17 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 17 pruned at epoch 14\n",
            "Validation Loss at prune: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:38:51,411] Trial 18 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 18 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:38:53,667] Trial 19 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 19 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:39:01,845] Trial 20 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 20 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:40:01,820] Trial 21 finished with value: 5.745785204197774e-05 and parameters: {'hidden_size': 226, 'num_layers': 2, 'learning_rate': 0.008906402630549978, 'batch_size': 46}. Best is trial 21 with value: 5.745785204197774e-05.\n",
            "[I 2024-09-29 10:40:04,766] Trial 22 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 22 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:40:54,332] Trial 23 finished with value: 6.116820726646206e-05 and parameters: {'hidden_size': 190, 'num_layers': 1, 'learning_rate': 0.007608006135586522, 'batch_size': 47}. Best is trial 21 with value: 5.745785204197774e-05.\n",
            "[I 2024-09-29 10:41:40,272] Trial 24 finished with value: 6.048202281011653e-05 and parameters: {'hidden_size': 180, 'num_layers': 1, 'learning_rate': 0.003185462494482815, 'batch_size': 51}. Best is trial 21 with value: 5.745785204197774e-05.\n",
            "[I 2024-09-29 10:41:42,927] Trial 25 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 25 pruned at epoch 14\n",
            "Validation Loss at prune: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:41:46,525] Trial 26 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 26 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:41:48,809] Trial 27 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 27 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 10:42:57,787] Trial 28 finished with value: 4.627792737121148e-05 and parameters: {'hidden_size': 180, 'num_layers': 1, 'learning_rate': 0.002162256483510343, 'batch_size': 30}. Best is trial 28 with value: 4.627792737121148e-05.\n",
            "[I 2024-09-29 10:43:02,004] Trial 29 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 29 pruned at epoch 10\n",
            "Validation Loss at prune: 0.0002\n",
            "Best hyperparameters:  {'hidden_size': 180, 'num_layers': 1, 'learning_rate': 0.002162256483510343, 'batch_size': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 하이퍼파라미터 저장"
      ],
      "metadata": {
        "id": "8GnVvI-7yNI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "### 하이퍼 파라미터 저장\n",
        "\n",
        "# 최적의 하이퍼파라미터를 JSON 파일로 저장\n",
        "with open(\"best_hyperparameters.json\", \"w\") as f:\n",
        "    json.dump(study.best_params, f, indent=4)\n",
        "\n",
        "print(\"Best hyperparameters have been saved to 'best_hyperparameters.json'\")"
      ],
      "metadata": {
        "id": "GxL7BNc0yQgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5342569d-bf3e-470a-e386-4a80c7d51a0d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters have been saved to 'best_hyperparameters.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습"
      ],
      "metadata": {
        "id": "bUE7hxpjv0-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 하이퍼파라미터 불러오기"
      ],
      "metadata": {
        "id": "pKnZirzYcyvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 저장된 하이퍼파라미터 불러오기\n",
        "with open(\"best_hyperparameters.json\", \"r\") as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "print(\"Loaded best hyperparameters:\", best_params)"
      ],
      "metadata": {
        "id": "JaBCcnz3cyZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 최적의 파라미터로 모델 학습"
      ],
      "metadata": {
        "id": "2gmm5kgcxyvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 10\n",
        "num_epochs = 200"
      ],
      "metadata": {
        "id": "250wfjS2xtYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 모델 학습\n",
        "best_params = study.best_params\n",
        "\n",
        "train_loader, val_loader, test_loader = create_dataloaders(tri, best_params['batch_size'], sequence_length)\n",
        "\n",
        "best_model = LSTM_Model(input_size=3, hidden_size=best_params['hidden_size'],\n",
        "                      num_layers=best_params['num_layers'], output_size=3)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
        "criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "veh0Ol9xhQYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "best_model.train()\n",
        "total_train_time = 0  # 총 학습 시간 기록 변수\n",
        "\n",
        "for epoch in range(num_epochs):  # 최종 모델 학습\n",
        "    best_model.train()\n",
        "    train_loss = 0.0\n",
        "    epoch_start_time = time.time()  # 에포크 시작 시간 기록\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x = batch_x.to(device)  # 입력 데이터 차원 조정\n",
        "        batch_y = batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = best_model(batch_x)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    epoch_end_time = time.time()  # 에포크 종료 시간 기록\n",
        "    epoch_duration = epoch_end_time - epoch_start_time  # 한 에포크의 학습 시간\n",
        "    total_train_time += epoch_duration  # 총 학습 시간에 추가\n",
        "\n",
        "    # Validation set에서 성능 측정\n",
        "    best_model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            batch_x = batch_x.to(device)  # 입력 데이터 차원 조정\n",
        "            batch_y = batch_y.to(device)\n",
        "            output = best_model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # 모델 저장\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "      model_save_path = f\"best_model_epoch_{epoch+1}.pth\"\n",
        "      torch.save(best_model.state_dict(), model_save_path)\n",
        "      print(f\"Model saved: {model_save_path}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_duration:.2f} seconds.\")\n",
        "\n",
        "print(f\"Total training time: {total_train_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "id": "CKJYU2CKhWZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 모델 요약 내용 출력"
      ],
      "metadata": {
        "id": "eCm9ArFSytv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(best_model, (1, 10, 3), device=device)"
      ],
      "metadata": {
        "id": "dtvUWEC8pdJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트"
      ],
      "metadata": {
        "id": "niiMUsYNy2Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 테스트 결과 생성"
      ],
      "metadata": {
        "id": "5-lJAmjGyx4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import torch\n",
        "\n",
        "# 모델 예측 및 성능 평가\n",
        "best_model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_loss = 0\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # 예측 수행\n",
        "        output = best_model(batch_x)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(output, batch_y)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # 예측값과 실제값 저장\n",
        "        y_pred_list.append(output.cpu().numpy())  # 예측값을 CPU로 이동하여 numpy 배열로 변환\n",
        "        y_true_list.append(batch_y.cpu().numpy())  # 실제값을 CPU로 이동하여 numpy 배열로 변환\n",
        "\n",
        "# 리스트를 numpy 배열로 변환\n",
        "y_pred = np.concatenate(y_pred_list, axis=0)\n",
        "y_true = np.concatenate(y_true_list, axis=0)\n",
        "\n",
        "# 성능 평가\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n"
      ],
      "metadata": {
        "id": "jZx_NHjT8ugM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 결과 시각화"
      ],
      "metadata": {
        "id": "vet2WrIsdUL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_visualization(y_true, y_pred)"
      ],
      "metadata": {
        "id": "xc_TdEFadJRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. raw 데이터와 predict 데이터 비교"
      ],
      "metadata": {
        "id": "F_CgxYsm9cJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_original = scaler.inverse_transform(y_pred)\n",
        "\n",
        "y_test_original = scaler.inverse_transform(y_true.reshape(-1, 3))\n",
        "\n",
        "prediction_visualization(y_test_original, y_pred_original)"
      ],
      "metadata": {
        "id": "HTosdwF2-dBe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
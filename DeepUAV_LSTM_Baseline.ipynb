{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m9WrRVLYlLra",
        "G_Y5diT0_CK5",
        "Ng0CFeAZyxl2",
        "1LTSh1vXy1fg",
        "eOFln2pmy4pt",
        "-rOFnV17y8Q-",
        "Sa6ydGidy_CI",
        "K90RnrEWB2Iw",
        "Jt5GdJ-uwNW8",
        "RsoiT-rUcsbi",
        "uILR-VuOtg1G",
        "wlwsWq26Gf5V",
        "XCmztXYWwA1z",
        "T8zpvEiavYtE",
        "zbQgifbFjDnj",
        "PfydFgp_qeMC",
        "8GnVvI-7yNI_",
        "bUE7hxpjv0-O",
        "niiMUsYNy2Xy",
        "5-lJAmjGyx4w"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jsXWnc9abSrT"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 패키지 설치 & 라이브러리 import"
      ],
      "metadata": {
        "id": "m9WrRVLYlLra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lPMKbVWF2Gq",
        "outputId": "25e475ff-5082-4817-eaf2-8e8f7c2660dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 colorlog-6.8.2 optuna-4.0.0\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "gKd7dfaTPzbm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전체 공통 함수"
      ],
      "metadata": {
        "id": "G_Y5diT0_CK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 데이터 보간 함수"
      ],
      "metadata": {
        "id": "Ng0CFeAZyxl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터의 시간 간격을 0.5초의 등간격으로 맞추고 그에 따른 좌표 값을 보간함\n",
        "- 가장 많이 사용되는 3차 스플라인 보간 적용"
      ],
      "metadata": {
        "id": "wk6vfDy4zsh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import interp1d\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 보간 - 위도, 고도, 경도 값을 0.5초 시간 간격으로 보간\n",
        "def spline_interpolation(x, y):\n",
        "    # x: 보간할 데이터의 x 값 (시간)\n",
        "    # y: 보간할 데이터의 y 값 (위도, 고도, 경도)\n",
        "\n",
        "    # 3차 스플라인 보간 함수 생성\n",
        "    f = interp1d(x, y, kind='cubic')\n",
        "\n",
        "    # 시간 범위 설정\n",
        "    start_time = min(x)\n",
        "    end_time = max(x)\n",
        "\n",
        "    # 0.5초 간격의 시간 배열을 생성\n",
        "    new_x = np.arange(start_time, end_time, 0.5)\n",
        "\n",
        "    # 위도, 경도, 고도 보간\n",
        "    new_y = f(new_x)\n",
        "\n",
        "    return new_x, new_y"
      ],
      "metadata": {
        "id": "MbOt2kIkQg_7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 시퀀스 나누는 함수"
      ],
      "metadata": {
        "id": "1LTSh1vXy1fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습을 위해 데이터를 시퀀스 형태로 나눔\n",
        "- look back과 forward에 따라 학습에 사용될 x값의 구간과 y값을 반환함\n",
        "- 초기 look back은 10, forward는 0이다."
      ],
      "metadata": {
        "id": "lWB9uZyCz1Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시퀀스 생성 함수\n",
        "# look back : 10 (5초간의 위도, 고도, 경도 데이터를 x값으로 사용)\n",
        "# forward : 0 (0.5초 뒤의 위도, 고도, 경도 데이터를 y값으로 사용)\n",
        "def create_sequences(df, look_back=10, forward=0):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for i in range(len(df) - look_back - forward):\n",
        "        x = df.iloc[i:(i+look_back)].values\n",
        "        y = df.iloc[i+look_back+forward].values\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)"
      ],
      "metadata": {
        "id": "_kmQ-_EyGFpW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 시간 소수점 절사 함수"
      ],
      "metadata": {
        "id": "eOFln2pmy4pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 불필요한 시간 소수점 제거"
      ],
      "metadata": {
        "id": "CDwOPPA05KGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# 시간 소수점 절사 함수\n",
        "def truncation(df):\n",
        "    new_time = []\n",
        "\n",
        "    for a in df['time']:\n",
        "      new_time.append(math.floor(a * 10) / 10)\n",
        "\n",
        "    df['time'] = new_time\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "CFiscMuBSycb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 스케일링 함수"
      ],
      "metadata": {
        "id": "-rOFnV17y8Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MinMaxScaling 적용 함수\n"
      ],
      "metadata": {
        "id": "442iH9qd_UIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# MinMaxScaling 적용 함수\n",
        "def min_max_scaling(df):\n",
        "\n",
        "    # time 삭제 df 생성\n",
        "    mid_df=df.drop(columns='time')\n",
        "\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.fit(mid_df)\n",
        "\n",
        "    min_max_data = min_max_scaler.transform(mid_df)\n",
        "\n",
        "    new_data = pd.DataFrame(min_max_data)\n",
        "\n",
        "    # time 추가\n",
        "    new_data.insert(0, 'time', df['time'])\n",
        "\n",
        "    # 나머지 열의 이름 가져오기\n",
        "    column_names = mid_df.columns.tolist()\n",
        "\n",
        "    column_names.insert(0, 'time')\n",
        "\n",
        "    print(column_names)\n",
        "\n",
        "    # 새로운 데이터프레임에 열 이름 설정\n",
        "    new_data.columns = column_names\n",
        "\n",
        "    return min_max_scaler, new_data"
      ],
      "metadata": {
        "id": "zR9B-lKYNgs8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. train/validation/test 분리 함수"
      ],
      "metadata": {
        "id": "Sa6ydGidy_CI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Train, Validation, Test 데이터셋을 분리하는 함수이다.\n",
        "- 각각의 비율은 6:2:2 이다."
      ],
      "metadata": {
        "id": "Ii6xjYd3A7YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train/validation/test 분리 함수\n",
        "def split_train_val_test(X, y):\n",
        "\n",
        "    #Train, Test 분류\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #Train, Validation 분류\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "    # # 데이터 크기 출력\n",
        "    # print(\"Train set:\", X_train.shape, y_train.shape)\n",
        "    # print(\"Validation set:\", X_val.shape, y_val.shape)\n",
        "    # print(\"Test set:\", X_test.shape, y_test.shape)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "metadata": {
        "id": "mXJlnnQq7FgL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 경로 시각화 함수"
      ],
      "metadata": {
        "id": "K90RnrEWB2Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 실제 경로와 예측 경로를 각각 그래프로 생성\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t2bNHfpSBPfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 경로 시각화 함수\n",
        "def prediction_visualization(y_test, y_pred):\n",
        "    # 시각화를 위해 위도, 경도, 고도 분리\n",
        "    y_test_lat = y_test[:, 0]\n",
        "    y_test_lon = y_test[:, 1]\n",
        "    y_test_alt = y_test[:, 2]\n",
        "\n",
        "    y_pred_lat = y_pred[:, 0]\n",
        "    y_pred_lon = y_pred[:, 1]\n",
        "    y_pred_alt = y_pred[:, 2]\n",
        "\n",
        "    # 3D 그래프 생성\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # 실제 값 플롯\n",
        "    ax.scatter(y_test_lat, y_test_lon, y_test_alt, c='b', marker='o', label='Actual')\n",
        "\n",
        "    # 예측 값 플롯\n",
        "    ax.scatter(y_pred_lat, y_pred_lon, y_pred_alt, c='r', marker='^', label='Predicted')\n",
        "\n",
        "    # 그래프 제목 및 축 레이블 설정\n",
        "    ax.set_title('Actual vs Predicted')\n",
        "    ax.set_xlabel('Latitude')\n",
        "    ax.set_ylabel('Longitude')\n",
        "    ax.set_zlabel('Altitude')\n",
        "\n",
        "    # 범례\n",
        "    ax.legend()\n",
        "\n",
        "    # 그래프를 화면에 출력\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "l8bKkBe-h6cG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 데이터로더 생성 함수"
      ],
      "metadata": {
        "id": "Jt5GdJ-uwNW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습에 사용할 데이터 로더 생성\n",
        "1. 시퀀스 생성\n",
        "2. Train, Validation, Test 분리\n",
        "3. DataLoader 생성"
      ],
      "metadata": {
        "id": "__Dcj9VbwfKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로더 생성 함수\n",
        "def create_dataloaders(tri, batch_size, sequence_length):\n",
        "\n",
        "  tri = tri[['time', 'lat', 'lon', 'alt']]\n",
        "\n",
        "  # 시퀀스 생성\n",
        "  train_x, train_y = create_sequences(tri[['lat', 'lon', 'alt']], sequence_length)\n",
        "\n",
        "  # train, validation, test 분리\n",
        "  X_train, X_val, X_test, y_train, y_val, y_test = split_train_val_test(train_x, train_y)\n",
        "\n",
        "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "  y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "  X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "  y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "  y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "  train_dataset = TensorDataset(X_train, y_train)\n",
        "  validation_dataset = TensorDataset(X_val, y_val)\n",
        "  test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "  # 데이터 로더 생성\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "IfsHAblYwU0v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "RsoiT-rUcsbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 데이터셋 가져오기"
      ],
      "metadata": {
        "id": "4YJd91PCtHJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 데이터셋 가져오기 (필요시 저장 경로 수정)\n",
        "raw = pd.read_csv(\"/content/OnboardGPS.csv\")"
      ],
      "metadata": {
        "id": "MuunCpw0crPo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 출력\n",
        "# raw"
      ],
      "metadata": {
        "id": "yEjs_s1FfNBG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 데이터셋 전처리"
      ],
      "metadata": {
        "id": "uILR-VuOtg1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터셋 보간\n",
        "- 소수점 절사\n",
        "- MinMaxScaling 적용\n"
      ],
      "metadata": {
        "id": "ea6emhMrtjFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tri = raw[['Timpstemp', ' lat', ' lon', ' alt']]\n",
        "\n",
        "new_x, lat = spline_interpolation(tri['Timpstemp'] / 1e6 , tri[' lat'])\n",
        "new_x, lon = spline_interpolation(tri['Timpstemp'] / 1e6 , tri[' lon'])\n",
        "new_x, alt = spline_interpolation(tri['Timpstemp'] / 1e6 , tri[' alt'])\n",
        "\n",
        "tri = pd.DataFrame({\"time\":new_x, \"lat\":lat, \"lon\":lon, \"alt\":alt})\n",
        "\n",
        "tri = truncation(tri)\n",
        "scaler, tri = min_max_scaling(tri)\n",
        "# tri"
      ],
      "metadata": {
        "id": "0M4Mw46Ge5dr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d91cc912-589a-42f4-ecda-60f7a0bf34e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['time', 'lat', 'lon', 'alt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 선언"
      ],
      "metadata": {
        "id": "wlwsWq26Gf5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 모델 클래스 선언"
      ],
      "metadata": {
        "id": "XCmztXYWwA1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LSTM을 사용한 모델 클래스 선언\n",
        "- 구조 : LSTM + Fully Connected Layer"
      ],
      "metadata": {
        "id": "Pk72DAdQvxkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import optuna\n",
        "\n",
        "# LSTM 모델 클래스 정의\n",
        "class LSTM_Model(nn.Module):\n",
        "    def __init__(self, input_size=3, hidden_size=128, num_layers=2, output_size=3):\n",
        "        super(LSTM_Model, self).__init__()\n",
        "\n",
        "        # hidden state size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTM layer 수\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # LSTM 레이어\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        # LSTM hidden state, cell state 초기화\n",
        "        # 초기 hidden state와 cell state 정의\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # LSTM 레이어를 통과\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # 마지막 시점의 hidden state에서 Fully Connected Layer를 통과\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "1-WDOIn-GiMc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 하이퍼파라미터 탐색"
      ],
      "metadata": {
        "id": "FYz0CNRn3w9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Optuna의 objective 함수 선언"
      ],
      "metadata": {
        "id": "T8zpvEiavYtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 최적의 하이퍼파라미터 탐색을 위해 optuna 사용\n",
        "- objective 함수 선언"
      ],
      "metadata": {
        "id": "7Tr7p_Llvd3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "\n",
        "def objective(trial, tri, sequence_length):\n",
        "    # 탐색할 하이퍼파라미터 값 결정\n",
        "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 5)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 16, 128)\n",
        "\n",
        "    num_epochs = 200\n",
        "    input_size = 3  # lat, lon, alt\n",
        "    output_size = 3  # lat, lon, alt\n",
        "\n",
        "    # 데이터셋 가져오기\n",
        "    train_loader, val_loader, _ = create_dataloaders(tri, batch_size, sequence_length)\n",
        "\n",
        "    # 모델 정의: LSTMModel\n",
        "    model = LSTM_Model(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    total_val_loss = 0.0  # 모든 epoch에서의 validation loss의 합\n",
        "    val_loss_count = 0    # 평균을 계산하기 위한 epoch 수\n",
        "\n",
        "    # 모델 학습\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Train set으로 학습\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation set에서 성능 측정\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x = batch_x.to(device)\n",
        "                batch_y = batch_y.to(device)\n",
        "                output = model(batch_x)\n",
        "                loss = criterion(output, batch_y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # 배치 수로 나눠서 평균 validation 손실 계산\n",
        "        val_loss_avg = val_loss / len(val_loader)\n",
        "\n",
        "        # Optuna에 중간 값 보고\n",
        "        trial.report(val_loss_avg, epoch)\n",
        "\n",
        "        # 총 validation 손실 계산\n",
        "        total_val_loss += val_loss_avg\n",
        "        val_loss_count += 1\n",
        "\n",
        "        # Pruning 조건 확인\n",
        "        if trial.should_prune():\n",
        "            print(f\"Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    # 모든 epoch에서의 validation 손실 평균을 반환\n",
        "    return total_val_loss / val_loss_count"
      ],
      "metadata": {
        "id": "ATdrPIwIgpHn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. early stopping 함수 선언"
      ],
      "metadata": {
        "id": "zbQgifbFjDnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- optuna에서 하이퍼파라미터 탐색 중 early stopping 적용"
      ],
      "metadata": {
        "id": "mAxKy5BBvnpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # https://github.com/optuna/optuna/issues/1001\n",
        "\n",
        "def early_stopping_check(study, trial, early_stopping_rounds=10):\n",
        "  current_trial_number = trial.number\n",
        "  best_trial_number = study.best_trial.number\n",
        "  should_stop = (current_trial_number - best_trial_number) >= early_stopping_rounds\n",
        "  if should_stop:\n",
        "      print(f\"Early stopping detected: stopping trial {current_trial_number} (best trial {best_trial_number}) at epoch {epoch}\")\n",
        "      study.stop()"
      ],
      "metadata": {
        "id": "Nzav0lS0jDAI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 탐색 범위 외의 하이퍼파라미터 선언"
      ],
      "metadata": {
        "id": "PfydFgp_qeMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 10\n",
        "early_stopping_rounds = 10\n",
        "n_trials = 30"
      ],
      "metadata": {
        "id": "FduoAs3iqfoW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 하이퍼 파라미터 탐색"
      ],
      "metadata": {
        "id": "-xyOYO-vyJxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from functools import partial\n",
        "\n",
        "# Optuna 스터디 생성 및 최적화 실행\n",
        "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
        "study.optimize(lambda trial: objective(trial, tri, sequence_length), callbacks=[partial(early_stopping_check, early_stopping_rounds=early_stopping_rounds)], n_trials=n_trials)\n",
        "\n",
        "# 최적의 하이퍼파라미터 출력\n",
        "print(\"Best hyperparameters: \", study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "BcflFUAU4q5p",
        "outputId": "1c5fc379-339e-457e-e0e6-e3d5b32898e0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-09-29 03:21:29,892] A new study created in memory with name: no-name-0ac206c4-d8a0-4ae0-bfba-69608cda7cd4\n",
            "[I 2024-09-29 03:22:42,419] Trial 0 finished with value: 0.0003085206865775945 and parameters: {'hidden_size': 48, 'num_layers': 1, 'learning_rate': 0.0030340011055910118, 'batch_size': 115}. Best is trial 0 with value: 0.0003085206865775945.\n",
            "[I 2024-09-29 03:27:43,183] Trial 1 finished with value: 0.024846066149607805 and parameters: {'hidden_size': 52, 'num_layers': 4, 'learning_rate': 1.6194172395895463e-05, 'batch_size': 103}. Best is trial 0 with value: 0.0003085206865775945.\n",
            "[W 2024-09-29 03:29:58,715] Trial 2 failed with parameters: {'hidden_size': 123, 'num_layers': 3, 'learning_rate': 2.188478344881274e-05, 'batch_size': 23} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-18-edda75ab7bef>\", line 7, in <lambda>\n",
            "    study.optimize(lambda trial: objective(trial, tri, sequence_length), callbacks=[partial(early_stopping_check, early_stopping_rounds=early_stopping_rounds)], n_trials=n_trials)\n",
            "  File \"<ipython-input-15-e7a683ec4a52>\", line 41, in objective\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 521, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 289, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 769, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "[W 2024-09-29 03:29:58,741] Trial 2 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-edda75ab7bef>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Optuna 스터디 생성 및 최적화 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 최적의 하이퍼파라미터 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-edda75ab7bef>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Optuna 스터디 생성 및 최적화 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 최적의 하이퍼파라미터 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-e7a683ec4a52>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, tri, sequence_length)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 하이퍼파라미터 저장"
      ],
      "metadata": {
        "id": "8GnVvI-7yNI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "### 하이퍼 파라미터 저장\n",
        "\n",
        "# 최적의 하이퍼파라미터를 JSON 파일로 저장\n",
        "with open(\"best_hyperparameters.json\", \"w\") as f:\n",
        "    json.dump(study.best_params, f, indent=4)\n",
        "\n",
        "print(\"Best hyperparameters have been saved to 'best_hyperparameters.json'\")"
      ],
      "metadata": {
        "id": "GxL7BNc0yQgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습"
      ],
      "metadata": {
        "id": "bUE7hxpjv0-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 하이퍼파라미터 불러오기"
      ],
      "metadata": {
        "id": "pKnZirzYcyvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 저장된 하이퍼파라미터 불러오기\n",
        "with open(\"best_hyperparameters.json\", \"r\") as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "print(\"Loaded best hyperparameters:\", best_params)"
      ],
      "metadata": {
        "id": "JaBCcnz3cyZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 최적의 파라미터로 모델 학습"
      ],
      "metadata": {
        "id": "2gmm5kgcxyvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 10\n",
        "num_epochs = 200"
      ],
      "metadata": {
        "id": "250wfjS2xtYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적의 모델 학습\n",
        "best_params = study.best_params\n",
        "\n",
        "train_loader, val_loader, test_loader = create_dataloaders(tri, best_params['batch_size'], sequence_length)\n",
        "\n",
        "best_model = LSTM_Model(input_size=3, hidden_size=best_params['hidden_size'],\n",
        "                      num_layers=best_params['num_layers'], output_size=3)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
        "criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "veh0Ol9xhQYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "best_model.train()\n",
        "total_train_time = 0  # 총 학습 시간 기록 변수\n",
        "\n",
        "for epoch in range(num_epochs):  # 최종 모델 학습\n",
        "    best_model.train()\n",
        "    train_loss = 0.0\n",
        "    epoch_start_time = time.time()  # 에포크 시작 시간 기록\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x = batch_x.to(device)  # 입력 데이터 차원 조정\n",
        "        batch_y = batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = best_model(batch_x)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    epoch_end_time = time.time()  # 에포크 종료 시간 기록\n",
        "    epoch_duration = epoch_end_time - epoch_start_time  # 한 에포크의 학습 시간\n",
        "    total_train_time += epoch_duration  # 총 학습 시간에 추가\n",
        "\n",
        "    # Validation set에서 성능 측정\n",
        "    best_model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            batch_x = batch_x.to(device)  # 입력 데이터 차원 조정\n",
        "            batch_y = batch_y.to(device)\n",
        "            output = best_model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # 모델 저장\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "      model_save_path = f\"best_model_epoch_{epoch+1}.pth\"\n",
        "      torch.save(best_model.state_dict(), model_save_path)\n",
        "      print(f\"Model saved: {model_save_path}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_duration:.2f} seconds.\")\n",
        "\n",
        "print(f\"Total training time: {total_train_time:.2f} seconds.\")\n"
      ],
      "metadata": {
        "id": "CKJYU2CKhWZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 모델 요약 내용 출력"
      ],
      "metadata": {
        "id": "eCm9ArFSytv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(best_model, (1, 10, 3), device=device)"
      ],
      "metadata": {
        "id": "dtvUWEC8pdJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트"
      ],
      "metadata": {
        "id": "niiMUsYNy2Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 테스트 결과 생성"
      ],
      "metadata": {
        "id": "5-lJAmjGyx4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import torch\n",
        "\n",
        "# 모델 예측 및 성능 평가\n",
        "best_model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_loss = 0\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # 예측 수행\n",
        "        output = best_model(batch_x)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(output, batch_y)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # 예측값과 실제값 저장\n",
        "        y_pred_list.append(output.cpu().numpy())  # 예측값을 CPU로 이동하여 numpy 배열로 변환\n",
        "        y_true_list.append(batch_y.cpu().numpy())  # 실제값을 CPU로 이동하여 numpy 배열로 변환\n",
        "\n",
        "# 리스트를 numpy 배열로 변환\n",
        "y_pred = np.concatenate(y_pred_list, axis=0)\n",
        "y_true = np.concatenate(y_true_list, axis=0)\n",
        "\n",
        "# 성능 평가\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n"
      ],
      "metadata": {
        "id": "jZx_NHjT8ugM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 결과 시각화"
      ],
      "metadata": {
        "id": "vet2WrIsdUL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_visualization(y_true, y_pred)"
      ],
      "metadata": {
        "id": "xc_TdEFadJRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. raw 데이터와 predict 데이터 비교"
      ],
      "metadata": {
        "id": "F_CgxYsm9cJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_original = scaler.inverse_transform(y_pred)\n",
        "\n",
        "y_test_original = scaler.inverse_transform(y_true.reshape(-1, 3))\n",
        "\n",
        "prediction_visualization(y_test_original, y_pred_original)"
      ],
      "metadata": {
        "id": "HTosdwF2-dBe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
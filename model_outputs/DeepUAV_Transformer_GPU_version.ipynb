{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsXWnc9abSrT",
        "outputId": "1f651520-f9c5-4db1-da46-b4db92bfec9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9WrRVLYlLra"
      },
      "source": [
        "### 패키지 설치 & 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lPMKbVWF2Gq",
        "outputId": "1cad2f15-c23a-424d-aba9-368a353e264c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.1)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 colorlog-6.8.2 optuna-4.0.0\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install optuna\n",
        "# !pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKd7dfaTPzbm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Y5diT0_CK5"
      },
      "source": [
        "## 전체 공통 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng0CFeAZyxl2"
      },
      "source": [
        "### 1. 데이터 보간 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6vfDy4zsh_"
      },
      "source": [
        "- 데이터의 시간 간격을 0.5초의 등간격으로 맞추고 그에 따른 좌표 값을 보간함\n",
        "- 가장 많이 사용되는 3차 스플라인 보간 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbOt2kIkQg_7"
      },
      "outputs": [],
      "source": [
        "from scipy.interpolate import interp1d\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 보간 - 위도, 고도, 경도 값을 0.5초 시간 간격으로 보간\n",
        "def spline_interpolation(x, y):\n",
        "    # x: 보간할 데이터의 x 값 (시간)\n",
        "    # y: 보간할 데이터의 y 값 (위도, 고도, 경도)\n",
        "\n",
        "    # 3차 스플라인 보간 함수 생성\n",
        "    f = interp1d(x, y, kind='cubic')\n",
        "\n",
        "    # 시간 범위 설정\n",
        "    start_time = min(x)\n",
        "    end_time = max(x)\n",
        "\n",
        "    # 0.5초 간격의 시간 배열을 생성\n",
        "    new_x = np.arange(start_time, end_time, 0.5)\n",
        "\n",
        "    # 위도, 경도, 고도 보간\n",
        "    new_y = f(new_x)\n",
        "\n",
        "    return new_x, new_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LTSh1vXy1fg"
      },
      "source": [
        "### 2. 시퀀스 나누는 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWB9uZyCz1Ro"
      },
      "source": [
        "- 학습을 위해 데이터를 시퀀스 형태로 나눔\n",
        "- look back과 forward에 따라 학습에 사용될 x값의 구간과 y값을 반환함\n",
        "- 초기 look back은 10, forward는 0이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kmQ-_EyGFpW"
      },
      "outputs": [],
      "source": [
        "# 시퀀스 생성 함수\n",
        "# look back : 10 (5초간의 위도, 고도, 경도 데이터를 x값으로 사용)\n",
        "# forward : 0 (0.5초 뒤의 위도, 고도, 경도 데이터를 y값으로 사용)\n",
        "def create_sequences(df, look_back=10, forward=0):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for i in range(len(df) - look_back - forward):\n",
        "        x = df.iloc[i:(i+look_back)].values\n",
        "        y = df.iloc[i+look_back+forward].values\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOFln2pmy4pt"
      },
      "source": [
        "### 3. 시간 소수점 절사 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDwOPPA05KGE"
      },
      "source": [
        "- 불필요한 시간 소수점 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFiscMuBSycb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# 시간 소수점 절사 함수\n",
        "def truncation(df):\n",
        "    new_time = []\n",
        "\n",
        "    for a in df['time']:\n",
        "      new_time.append(math.floor(a * 10) / 10)\n",
        "\n",
        "    df['time'] = new_time\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rOFnV17y8Q-"
      },
      "source": [
        "### 4. 스케일링 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "442iH9qd_UIt"
      },
      "source": [
        "- MinMaxScaling 적용 함수\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR9B-lKYNgs8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# MinMaxScaling 적용 함수\n",
        "def min_max_scaling(df):\n",
        "\n",
        "    # time 삭제 df 생성\n",
        "    mid_df=df.drop(columns='time')\n",
        "\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.fit(mid_df)\n",
        "\n",
        "    min_max_data = min_max_scaler.transform(mid_df)\n",
        "\n",
        "    new_data = pd.DataFrame(min_max_data)\n",
        "\n",
        "    # time 추가\n",
        "    new_data.insert(0, 'time', df['time'])\n",
        "\n",
        "    # 나머지 열의 이름 가져오기\n",
        "    column_names = mid_df.columns.tolist()\n",
        "\n",
        "    column_names.insert(0, 'time')\n",
        "\n",
        "    print(column_names)\n",
        "\n",
        "    # 새로운 데이터프레임에 열 이름 설정\n",
        "    new_data.columns = column_names\n",
        "\n",
        "    return min_max_scaler, new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa6ydGidy_CI"
      },
      "source": [
        "### 5. train/validation/test 분리 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6xjYd3A7YK"
      },
      "source": [
        "- Train, Validation, Test 데이터셋을 분리하는 함수이다.\n",
        "- 각각의 비율은 6:2:2 이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXJlnnQq7FgL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train/validation/test 분리 함수\n",
        "def split_train_val_test(df, test_size=0.2, val_size=0.25):\n",
        "\n",
        "    #Train, Test 분류\n",
        "    train_val_df, test_df = train_test_split(df, test_size=test_size, shuffle=False)\n",
        "\n",
        "    #Train, Validation 분류\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=val_size, shuffle=False)\n",
        "\n",
        "    # # 데이터 크기 출력\n",
        "    # print(\"Train set:\", train_df.shape)\n",
        "    # print(\"Validation set:\", val_df.shape)\n",
        "    # print(\"Test set:\", test_df.shape)\n",
        "\n",
        "    return train_df, val_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K90RnrEWB2Iw"
      },
      "source": [
        "### 6. 경로 시각화 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2bNHfpSBPfr"
      },
      "source": [
        "- 실제 경로와 예측 경로를 각각 그래프로 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8bKkBe-h6cG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 경로 시각화 함수\n",
        "def prediction_visualization(y_test, y_pred):\n",
        "    # 시각화를 위해 위도, 경도, 고도 분리\n",
        "    y_test_lat = y_test[:, 0]\n",
        "    y_test_lon = y_test[:, 1]\n",
        "    y_test_alt = y_test[:, 2]\n",
        "\n",
        "    y_pred_lat = y_pred[:, 0]\n",
        "    y_pred_lon = y_pred[:, 1]\n",
        "    y_pred_alt = y_pred[:, 2]\n",
        "\n",
        "    # 3D 그래프 생성\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # 실제 값 플롯\n",
        "    ax.scatter(y_test_lat, y_test_lon, y_test_alt, c='b', marker='o', label='Actual')\n",
        "\n",
        "    # 예측 값 플롯\n",
        "    ax.scatter(y_pred_lat, y_pred_lon, y_pred_alt, c='r', marker='^', label='Predicted')\n",
        "\n",
        "    # 그래프 제목 및 축 레이블 설정\n",
        "    ax.set_title('Actual vs Predicted')\n",
        "    ax.set_xlabel('Latitude')\n",
        "    ax.set_ylabel('Longitude')\n",
        "    ax.set_zlabel('Altitude')\n",
        "\n",
        "    # 범례\n",
        "    ax.legend()\n",
        "\n",
        "    # 그래프를 화면에 출력\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_9fg_Gt0b73"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# splited 데이터 경로 시각화 함수\n",
        "def plot_trajectory(train_data, val_data, test_data):\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # 훈련 데이터 플롯\n",
        "    ax.plot(train_data['lat'], train_data['lon'], train_data['alt'], label='Train', color='blue')\n",
        "\n",
        "    # 검증 데이터 플롯\n",
        "    ax.plot(val_data['lat'], val_data['lon'], val_data['alt'], label='Validation', color='green')\n",
        "\n",
        "    # 테스트 데이터 플롯\n",
        "    ax.plot(test_data['lat'], test_data['lon'], test_data['alt'], label='Test', color='red')\n",
        "\n",
        "    # 그래프 레이블\n",
        "    ax.set_xlabel('Latitude')\n",
        "    ax.set_ylabel('Longitude')\n",
        "    ax.set_zlabel('Altitude')\n",
        "\n",
        "    # 제목과 범례\n",
        "    ax.set_title('Drone Trajectory: Train, Validation, and Test')\n",
        "    ax.legend()\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hesydw366u6"
      },
      "source": [
        "### 7. 데이터로더 생성 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTyDCnRc66u6"
      },
      "source": [
        "- 학습에 사용할 데이터 로더 생성\n",
        "1. 시퀀스 생성\n",
        "2. Train, Validation, Test 분리\n",
        "3. DataLoader 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTbRbKmU66u6"
      },
      "outputs": [],
      "source": [
        "# 데이터 로더 생성 함수\n",
        "def create_dataloaders(tri, batch_size, sequence_length):\n",
        "\n",
        "  tri = tri[['time', 'lat', 'lon', 'alt']]\n",
        "\n",
        "  # train, validation, test 분리\n",
        "  train_df, val_df, test_df = split_train_val_test(tri[['lat', 'lon', 'alt']])\n",
        "\n",
        "  # 각 데이터에 대해 시퀀스 생성\n",
        "  train_x, train_y = create_sequences(train_df, sequence_length)\n",
        "  val_x, val_y = create_sequences(val_df, sequence_length)\n",
        "  test_x, test_y = create_sequences(test_df, sequence_length)\n",
        "\n",
        "  X_train = torch.tensor(train_x, dtype=torch.float32)\n",
        "  y_train = torch.tensor(train_y, dtype=torch.float32)\n",
        "  X_val = torch.tensor(val_x, dtype=torch.float32)\n",
        "  y_val = torch.tensor(val_y, dtype=torch.float32)\n",
        "  X_test = torch.tensor(test_x, dtype=torch.float32)\n",
        "  y_test = torch.tensor(test_y, dtype=torch.float32)\n",
        "\n",
        "  train_dataset = TensorDataset(X_train, y_train)\n",
        "  validation_dataset = TensorDataset(X_val, y_val)\n",
        "  test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "  # 데이터 로더 생성\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsoiT-rUcsbi"
      },
      "source": [
        "## 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YJd91PCtHJS"
      },
      "source": [
        "### 1. 데이터셋 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuunCpw0crPo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 데이터셋 가져오기 (필요시 저장 경로 수정)\n",
        "raw = pd.read_csv(\"OnboardGPS.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uILR-VuOtg1G"
      },
      "source": [
        "### 2. 데이터셋 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea6emhMrtjFn"
      },
      "source": [
        "- 데이터셋 보간\n",
        "- 소수점 절사\n",
        "- MinMaxScaling 적용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M4Mw46Ge5dr",
        "outputId": "ad8d35b6-033f-4454-d72e-49801ebb73b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['time', 'lat', 'lon', 'alt']\n"
          ]
        }
      ],
      "source": [
        "tri = raw[['Timpstemp', ' lat', ' lon', ' alt']]\n",
        "\n",
        "new_x, lat = spline_interpolation(tri['Timpstemp'] / 1e6 , tri[' lat'])\n",
        "new_x, lon = spline_interpolation(tri['Timpstemp'] / 1e6 , tri[' lon'])\n",
        "new_x, alt = spline_interpolation(tri['Timpstemp'] / 1e6 , tri[' alt'])\n",
        "\n",
        "tri = pd.DataFrame({\"time\":new_x, \"lat\":lat, \"lon\":lon, \"alt\":alt})\n",
        "\n",
        "tri = truncation(tri)\n",
        "scaler, tri = min_max_scaling(tri)\n",
        "# tri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlwsWq26Gf5V"
      },
      "source": [
        "## 모델 선언"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCmztXYWwA1z"
      },
      "source": [
        "### 1. 모델 클래스 선언\n",
        "- Transformer를 사용한 모델 클래스 선언\n",
        "- 구조 : Transformer + Positional Encoding + Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATdrPIwIgpHn"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        if d_model % 2 != 0:\n",
        "            raise ValueError(f\"d_model must be even, but got {d_model}\")\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-WDOIn-GiMc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, seq_length, d_model, nhead, num_encoder_layers, dim_feedforward):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, d_model)  # input_dim에서 d_model로 변환\n",
        "        self.positional_encoding = PositionalEncoding(d_model)  # 포지셔널 인코딩 추가\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=d_model,\n",
        "                nhead=nhead,\n",
        "                dim_feedforward=dim_feedforward,\n",
        "                dropout=0.0 ),\n",
        "            num_layers=num_encoder_layers\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, input_dim)  # d_model에서 input_dim으로 변환\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)  # 입력 차원 -> d_model로 변환\n",
        "        src = self.positional_encoding(src)\n",
        "        src = src.permute(1, 0, 2)  # (batch_size, seq_length, d_model) -> (seq_length, batch_size, d_model)\n",
        "        output = self.transformer_encoder(src)  # Transformer Encoder 통과\n",
        "        output = self.fc_out(output)  # (seq_length, batch_size, d_model) -> (seq_length, batch_size, input_dim)\n",
        "\n",
        "        # 마지막 시점의 출력만 반환\n",
        "        return output[-1, :, :]  # shape: (batch_size, input_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYz0CNRn3w9P"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eohY6qs0xg2O"
      },
      "source": [
        "### 1. 하이퍼파라미터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNJ0UWidblTl",
        "outputId": "b54d5bf3-67b1-48e8-c29f-6a32f6acf346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best hyperparameters: {'nhead': 6, 'd_model': 270, 'num_encoder_layers': 2, 'dim_feedforward': 357, 'learning_rate': 0.00024457427461174166, 'batch_size': 37}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# 저장된 하이퍼파라미터 불러오기\n",
        "with open(\"best_hyperparameters.json\", \"r\") as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "print(\"Loaded best hyperparameters:\", best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdznBzKYx31i"
      },
      "source": [
        "### 2. 최적의 파라미터로 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nfvmpVT66vA"
      },
      "outputs": [],
      "source": [
        "sequence_length = 10\n",
        "num_epochs = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veh0Ol9xhQYV",
        "outputId": "fb7ef8f6-bd28-46a2-b2cc-7112c379fde3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# 최적의 모델 학습\n",
        "# best_params = study.best_params\n",
        "\n",
        "train_loader, val_loader, test_loader = create_dataloaders(tri, best_params['batch_size'], sequence_length)\n",
        "\n",
        "best_model = TransformerModel(\n",
        "    input_dim=3,\n",
        "    seq_length=10,\n",
        "    d_model=best_params['d_model'],\n",
        "    nhead=best_params['nhead'],\n",
        "    num_encoder_layers=best_params['num_encoder_layers'],\n",
        "    dim_feedforward=best_params['dim_feedforward']\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKJYU2CKhWZE",
        "outputId": "5a243dce-e0f5-4663-a267-1ce0ce71d621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Train Loss: 1.4217, Val Loss: 0.0244\n",
            "Epoch 1/300 completed in 0.72 seconds.\n",
            "Epoch [2/300], Train Loss: 0.0242, Val Loss: 0.0120\n",
            "Epoch 2/300 completed in 0.73 seconds.\n",
            "Epoch [3/300], Train Loss: 0.0187, Val Loss: 0.0090\n",
            "Epoch 3/300 completed in 0.72 seconds.\n",
            "Epoch [4/300], Train Loss: 0.0286, Val Loss: 0.0112\n",
            "Epoch 4/300 completed in 0.59 seconds.\n",
            "Epoch [5/300], Train Loss: 0.0173, Val Loss: 0.0091\n",
            "Epoch 5/300 completed in 0.53 seconds.\n",
            "Epoch [6/300], Train Loss: 0.0170, Val Loss: 0.0045\n",
            "Epoch 6/300 completed in 0.51 seconds.\n",
            "Epoch [7/300], Train Loss: 0.0172, Val Loss: 0.0153\n",
            "Epoch 7/300 completed in 0.52 seconds.\n",
            "Epoch [8/300], Train Loss: 0.0222, Val Loss: 0.0087\n",
            "Epoch 8/300 completed in 0.51 seconds.\n",
            "Epoch [9/300], Train Loss: 0.0201, Val Loss: 0.0076\n",
            "Epoch 9/300 completed in 0.49 seconds.\n",
            "Epoch [10/300], Train Loss: 0.0157, Val Loss: 0.0282\n",
            "Epoch 10/300 completed in 0.51 seconds.\n",
            "Epoch [11/300], Train Loss: 0.0242, Val Loss: 0.0060\n",
            "Epoch 11/300 completed in 0.50 seconds.\n",
            "Epoch [12/300], Train Loss: 0.0166, Val Loss: 0.0103\n",
            "Epoch 12/300 completed in 0.51 seconds.\n",
            "Epoch [13/300], Train Loss: 0.0155, Val Loss: 0.0112\n",
            "Epoch 13/300 completed in 0.51 seconds.\n",
            "Epoch [14/300], Train Loss: 0.0142, Val Loss: 0.0053\n",
            "Epoch 14/300 completed in 0.50 seconds.\n",
            "Epoch [15/300], Train Loss: 0.0118, Val Loss: 0.0037\n",
            "Epoch 15/300 completed in 0.49 seconds.\n",
            "Epoch [16/300], Train Loss: 0.0184, Val Loss: 0.0331\n",
            "Epoch 16/300 completed in 0.50 seconds.\n",
            "Epoch [17/300], Train Loss: 0.0177, Val Loss: 0.0093\n",
            "Epoch 17/300 completed in 0.52 seconds.\n",
            "Epoch [18/300], Train Loss: 0.0120, Val Loss: 0.0049\n",
            "Epoch 18/300 completed in 0.51 seconds.\n",
            "Epoch [19/300], Train Loss: 0.0161, Val Loss: 0.0089\n",
            "Epoch 19/300 completed in 0.50 seconds.\n",
            "Epoch [20/300], Train Loss: 0.0127, Val Loss: 0.0069\n",
            "Epoch 20/300 completed in 0.49 seconds.\n",
            "Epoch [21/300], Train Loss: 0.0098, Val Loss: 0.0063\n",
            "Epoch 21/300 completed in 0.51 seconds.\n",
            "Epoch [22/300], Train Loss: 0.0121, Val Loss: 0.0080\n",
            "Epoch 22/300 completed in 0.67 seconds.\n",
            "Epoch [23/300], Train Loss: 0.0128, Val Loss: 0.0049\n",
            "Epoch 23/300 completed in 0.67 seconds.\n",
            "Epoch [24/300], Train Loss: 0.0147, Val Loss: 0.0036\n",
            "Epoch 24/300 completed in 0.73 seconds.\n",
            "Epoch [25/300], Train Loss: 0.0155, Val Loss: 0.0144\n",
            "Epoch 25/300 completed in 0.63 seconds.\n",
            "Epoch [26/300], Train Loss: 0.0094, Val Loss: 0.0040\n",
            "Epoch 26/300 completed in 0.52 seconds.\n",
            "Epoch [27/300], Train Loss: 0.0104, Val Loss: 0.0025\n",
            "Epoch 27/300 completed in 0.53 seconds.\n",
            "Epoch [28/300], Train Loss: 0.0185, Val Loss: 0.0066\n",
            "Epoch 28/300 completed in 0.51 seconds.\n",
            "Epoch [29/300], Train Loss: 0.0143, Val Loss: 0.0023\n",
            "Epoch 29/300 completed in 0.51 seconds.\n",
            "Epoch [30/300], Train Loss: 0.0120, Val Loss: 0.0044\n",
            "Epoch 30/300 completed in 0.50 seconds.\n",
            "Epoch [31/300], Train Loss: 0.0074, Val Loss: 0.0057\n",
            "Epoch 31/300 completed in 0.51 seconds.\n",
            "Epoch [32/300], Train Loss: 0.0109, Val Loss: 0.0058\n",
            "Epoch 32/300 completed in 0.50 seconds.\n",
            "Epoch [33/300], Train Loss: 0.0157, Val Loss: 0.0069\n",
            "Epoch 33/300 completed in 0.51 seconds.\n",
            "Epoch [34/300], Train Loss: 0.0189, Val Loss: 0.0045\n",
            "Epoch 34/300 completed in 0.50 seconds.\n",
            "Epoch [35/300], Train Loss: 0.0133, Val Loss: 0.0030\n",
            "Epoch 35/300 completed in 0.51 seconds.\n",
            "Epoch [36/300], Train Loss: 0.0137, Val Loss: 0.0022\n",
            "Epoch 36/300 completed in 0.50 seconds.\n",
            "Epoch [37/300], Train Loss: 0.0117, Val Loss: 0.0084\n",
            "Epoch 37/300 completed in 0.50 seconds.\n",
            "Epoch [38/300], Train Loss: 0.0117, Val Loss: 0.0060\n",
            "Epoch 38/300 completed in 0.51 seconds.\n",
            "Epoch [39/300], Train Loss: 0.0076, Val Loss: 0.0085\n",
            "Epoch 39/300 completed in 0.49 seconds.\n",
            "Epoch [40/300], Train Loss: 0.0113, Val Loss: 0.0023\n",
            "Epoch 40/300 completed in 0.50 seconds.\n",
            "Epoch [41/300], Train Loss: 0.0131, Val Loss: 0.0173\n",
            "Epoch 41/300 completed in 0.51 seconds.\n",
            "Epoch [42/300], Train Loss: 0.0098, Val Loss: 0.0044\n",
            "Epoch 42/300 completed in 0.52 seconds.\n",
            "Epoch [43/300], Train Loss: 0.0103, Val Loss: 0.0064\n",
            "Epoch 43/300 completed in 0.67 seconds.\n",
            "Epoch [44/300], Train Loss: 0.0136, Val Loss: 0.0180\n",
            "Epoch 44/300 completed in 0.64 seconds.\n",
            "Epoch [45/300], Train Loss: 0.0153, Val Loss: 0.0056\n",
            "Epoch 45/300 completed in 0.71 seconds.\n",
            "Epoch [46/300], Train Loss: 0.0103, Val Loss: 0.0061\n",
            "Epoch 46/300 completed in 0.68 seconds.\n",
            "Epoch [47/300], Train Loss: 0.0112, Val Loss: 0.0065\n",
            "Epoch 47/300 completed in 0.53 seconds.\n",
            "Epoch [48/300], Train Loss: 0.0094, Val Loss: 0.0017\n",
            "Epoch 48/300 completed in 0.51 seconds.\n",
            "Epoch [49/300], Train Loss: 0.0080, Val Loss: 0.0057\n",
            "Epoch 49/300 completed in 0.50 seconds.\n",
            "Epoch [50/300], Train Loss: 0.0117, Val Loss: 0.0084\n",
            "Epoch 50/300 completed in 0.51 seconds.\n",
            "Epoch [51/300], Train Loss: 0.0125, Val Loss: 0.0068\n",
            "Epoch 51/300 completed in 0.52 seconds.\n",
            "Epoch [52/300], Train Loss: 0.0087, Val Loss: 0.0059\n",
            "Epoch 52/300 completed in 0.52 seconds.\n",
            "Epoch [53/300], Train Loss: 0.0070, Val Loss: 0.0022\n",
            "Epoch 53/300 completed in 0.50 seconds.\n",
            "Epoch [54/300], Train Loss: 0.0085, Val Loss: 0.0055\n",
            "Epoch 54/300 completed in 0.49 seconds.\n",
            "Epoch [55/300], Train Loss: 0.0122, Val Loss: 0.0069\n",
            "Epoch 55/300 completed in 0.51 seconds.\n",
            "Epoch [56/300], Train Loss: 0.0123, Val Loss: 0.0033\n",
            "Epoch 56/300 completed in 0.50 seconds.\n",
            "Epoch [57/300], Train Loss: 0.0059, Val Loss: 0.0043\n",
            "Epoch 57/300 completed in 0.52 seconds.\n",
            "Epoch [58/300], Train Loss: 0.0082, Val Loss: 0.0024\n",
            "Epoch 58/300 completed in 0.50 seconds.\n",
            "Epoch [59/300], Train Loss: 0.0106, Val Loss: 0.0066\n",
            "Epoch 59/300 completed in 0.51 seconds.\n",
            "Epoch [60/300], Train Loss: 0.0131, Val Loss: 0.0050\n",
            "Epoch 60/300 completed in 0.50 seconds.\n",
            "Epoch [61/300], Train Loss: 0.0072, Val Loss: 0.0054\n",
            "Epoch 61/300 completed in 0.51 seconds.\n",
            "Epoch [62/300], Train Loss: 0.0090, Val Loss: 0.0017\n",
            "Epoch 62/300 completed in 0.50 seconds.\n",
            "Epoch [63/300], Train Loss: 0.0108, Val Loss: 0.0055\n",
            "Epoch 63/300 completed in 0.51 seconds.\n",
            "Epoch [64/300], Train Loss: 0.0091, Val Loss: 0.0062\n",
            "Epoch 64/300 completed in 0.64 seconds.\n",
            "Epoch [65/300], Train Loss: 0.0092, Val Loss: 0.0016\n",
            "Epoch 65/300 completed in 0.64 seconds.\n",
            "Epoch [66/300], Train Loss: 0.0085, Val Loss: 0.0032\n",
            "Epoch 66/300 completed in 0.72 seconds.\n",
            "Epoch [67/300], Train Loss: 0.0085, Val Loss: 0.0052\n",
            "Epoch 67/300 completed in 0.65 seconds.\n",
            "Epoch [68/300], Train Loss: 0.0093, Val Loss: 0.0068\n",
            "Epoch 68/300 completed in 0.53 seconds.\n",
            "Epoch [69/300], Train Loss: 0.0114, Val Loss: 0.0048\n",
            "Epoch 69/300 completed in 0.52 seconds.\n",
            "Epoch [70/300], Train Loss: 0.0072, Val Loss: 0.0019\n",
            "Epoch 70/300 completed in 0.51 seconds.\n",
            "Epoch [71/300], Train Loss: 0.0089, Val Loss: 0.0087\n",
            "Epoch 71/300 completed in 0.50 seconds.\n",
            "Epoch [72/300], Train Loss: 0.0106, Val Loss: 0.0022\n",
            "Epoch 72/300 completed in 0.52 seconds.\n",
            "Epoch [73/300], Train Loss: 0.0098, Val Loss: 0.0040\n",
            "Epoch 73/300 completed in 0.50 seconds.\n",
            "Epoch [74/300], Train Loss: 0.0056, Val Loss: 0.0012\n",
            "Epoch 74/300 completed in 0.51 seconds.\n",
            "Epoch [75/300], Train Loss: 0.0071, Val Loss: 0.0026\n",
            "Epoch 75/300 completed in 0.50 seconds.\n",
            "Epoch [76/300], Train Loss: 0.0052, Val Loss: 0.0026\n",
            "Epoch 76/300 completed in 0.51 seconds.\n",
            "Epoch [77/300], Train Loss: 0.0075, Val Loss: 0.0013\n",
            "Epoch 77/300 completed in 0.51 seconds.\n",
            "Epoch [78/300], Train Loss: 0.0099, Val Loss: 0.0055\n",
            "Epoch 78/300 completed in 0.54 seconds.\n",
            "Epoch [79/300], Train Loss: 0.0094, Val Loss: 0.0052\n",
            "Epoch 79/300 completed in 0.50 seconds.\n",
            "Epoch [80/300], Train Loss: 0.0081, Val Loss: 0.0028\n",
            "Epoch 80/300 completed in 0.51 seconds.\n",
            "Epoch [81/300], Train Loss: 0.0072, Val Loss: 0.0026\n",
            "Epoch 81/300 completed in 0.50 seconds.\n",
            "Epoch [82/300], Train Loss: 0.0068, Val Loss: 0.0033\n",
            "Epoch 82/300 completed in 0.49 seconds.\n",
            "Epoch [83/300], Train Loss: 0.0057, Val Loss: 0.0036\n",
            "Epoch 83/300 completed in 0.51 seconds.\n",
            "Epoch [84/300], Train Loss: 0.0074, Val Loss: 0.0021\n",
            "Epoch 84/300 completed in 0.51 seconds.\n",
            "Epoch [85/300], Train Loss: 0.0068, Val Loss: 0.0021\n",
            "Epoch 85/300 completed in 0.67 seconds.\n",
            "Epoch [86/300], Train Loss: 0.0068, Val Loss: 0.0137\n",
            "Epoch 86/300 completed in 0.66 seconds.\n",
            "Epoch [87/300], Train Loss: 0.0091, Val Loss: 0.0024\n",
            "Epoch 87/300 completed in 0.72 seconds.\n",
            "Epoch [88/300], Train Loss: 0.0079, Val Loss: 0.0036\n",
            "Epoch 88/300 completed in 0.64 seconds.\n",
            "Epoch [89/300], Train Loss: 0.0064, Val Loss: 0.0026\n",
            "Epoch 89/300 completed in 0.52 seconds.\n",
            "Epoch [90/300], Train Loss: 0.0100, Val Loss: 0.0013\n",
            "Epoch 90/300 completed in 0.50 seconds.\n",
            "Epoch [91/300], Train Loss: 0.0051, Val Loss: 0.0060\n",
            "Epoch 91/300 completed in 0.51 seconds.\n",
            "Epoch [92/300], Train Loss: 0.0100, Val Loss: 0.0094\n",
            "Epoch 92/300 completed in 0.51 seconds.\n",
            "Epoch [93/300], Train Loss: 0.0091, Val Loss: 0.0041\n",
            "Epoch 93/300 completed in 0.52 seconds.\n",
            "Epoch [94/300], Train Loss: 0.0050, Val Loss: 0.0078\n",
            "Epoch 94/300 completed in 0.49 seconds.\n",
            "Epoch [95/300], Train Loss: 0.0067, Val Loss: 0.0023\n",
            "Epoch 95/300 completed in 0.52 seconds.\n",
            "Epoch [96/300], Train Loss: 0.0069, Val Loss: 0.0012\n",
            "Epoch 96/300 completed in 0.50 seconds.\n",
            "Epoch [97/300], Train Loss: 0.0063, Val Loss: 0.0026\n",
            "Epoch 97/300 completed in 0.52 seconds.\n",
            "Epoch [98/300], Train Loss: 0.0077, Val Loss: 0.0055\n",
            "Epoch 98/300 completed in 0.50 seconds.\n",
            "Epoch [99/300], Train Loss: 0.0078, Val Loss: 0.0027\n",
            "Epoch 99/300 completed in 0.50 seconds.\n",
            "Epoch [100/300], Train Loss: 0.0057, Val Loss: 0.0051\n",
            "Epoch 100/300 completed in 0.51 seconds.\n",
            "Epoch [101/300], Train Loss: 0.0069, Val Loss: 0.0089\n",
            "Epoch 101/300 completed in 0.49 seconds.\n",
            "Epoch [102/300], Train Loss: 0.0086, Val Loss: 0.0053\n",
            "Epoch 102/300 completed in 0.53 seconds.\n",
            "Epoch [103/300], Train Loss: 0.0049, Val Loss: 0.0017\n",
            "Epoch 103/300 completed in 0.49 seconds.\n",
            "Epoch [104/300], Train Loss: 0.0078, Val Loss: 0.0098\n",
            "Epoch 104/300 completed in 0.51 seconds.\n",
            "Epoch [105/300], Train Loss: 0.0096, Val Loss: 0.0028\n",
            "Epoch 105/300 completed in 0.50 seconds.\n",
            "Epoch [106/300], Train Loss: 0.0061, Val Loss: 0.0033\n",
            "Epoch 106/300 completed in 0.65 seconds.\n",
            "Epoch [107/300], Train Loss: 0.0044, Val Loss: 0.0032\n",
            "Epoch 107/300 completed in 0.66 seconds.\n",
            "Epoch [108/300], Train Loss: 0.0067, Val Loss: 0.0026\n",
            "Epoch 108/300 completed in 0.71 seconds.\n",
            "Epoch [109/300], Train Loss: 0.0062, Val Loss: 0.0022\n",
            "Epoch 109/300 completed in 0.63 seconds.\n",
            "Epoch [110/300], Train Loss: 0.0050, Val Loss: 0.0040\n",
            "Epoch 110/300 completed in 0.51 seconds.\n",
            "Epoch [111/300], Train Loss: 0.0060, Val Loss: 0.0013\n",
            "Epoch 111/300 completed in 0.51 seconds.\n",
            "Epoch [112/300], Train Loss: 0.0078, Val Loss: 0.0021\n",
            "Epoch 112/300 completed in 0.51 seconds.\n",
            "Epoch [113/300], Train Loss: 0.0051, Val Loss: 0.0028\n",
            "Epoch 113/300 completed in 0.50 seconds.\n",
            "Epoch [114/300], Train Loss: 0.0055, Val Loss: 0.0025\n",
            "Epoch 114/300 completed in 0.50 seconds.\n",
            "Epoch [115/300], Train Loss: 0.0069, Val Loss: 0.0064\n",
            "Epoch 115/300 completed in 0.51 seconds.\n",
            "Epoch [116/300], Train Loss: 0.0093, Val Loss: 0.0052\n",
            "Epoch 116/300 completed in 0.50 seconds.\n",
            "Epoch [117/300], Train Loss: 0.0065, Val Loss: 0.0042\n",
            "Epoch 117/300 completed in 0.51 seconds.\n",
            "Epoch [118/300], Train Loss: 0.0055, Val Loss: 0.0025\n",
            "Epoch 118/300 completed in 0.49 seconds.\n",
            "Epoch [119/300], Train Loss: 0.0065, Val Loss: 0.0022\n",
            "Epoch 119/300 completed in 0.51 seconds.\n",
            "Epoch [120/300], Train Loss: 0.0045, Val Loss: 0.0011\n",
            "Epoch 120/300 completed in 0.52 seconds.\n",
            "Epoch [121/300], Train Loss: 0.0052, Val Loss: 0.0013\n",
            "Epoch 121/300 completed in 0.52 seconds.\n",
            "Epoch [122/300], Train Loss: 0.0057, Val Loss: 0.0030\n",
            "Epoch 122/300 completed in 0.50 seconds.\n",
            "Epoch [123/300], Train Loss: 0.0043, Val Loss: 0.0023\n",
            "Epoch 123/300 completed in 0.50 seconds.\n",
            "Epoch [124/300], Train Loss: 0.0050, Val Loss: 0.0077\n",
            "Epoch 124/300 completed in 0.49 seconds.\n",
            "Epoch [125/300], Train Loss: 0.0065, Val Loss: 0.0043\n",
            "Epoch 125/300 completed in 0.51 seconds.\n",
            "Epoch [126/300], Train Loss: 0.0067, Val Loss: 0.0040\n",
            "Epoch 126/300 completed in 0.51 seconds.\n",
            "Epoch [127/300], Train Loss: 0.0058, Val Loss: 0.0020\n",
            "Epoch 127/300 completed in 0.65 seconds.\n",
            "Epoch [128/300], Train Loss: 0.0041, Val Loss: 0.0025\n",
            "Epoch 128/300 completed in 0.65 seconds.\n",
            "Epoch [129/300], Train Loss: 0.0069, Val Loss: 0.0110\n",
            "Epoch 129/300 completed in 0.77 seconds.\n",
            "Epoch [130/300], Train Loss: 0.0083, Val Loss: 0.0033\n",
            "Epoch 130/300 completed in 0.62 seconds.\n",
            "Epoch [131/300], Train Loss: 0.0066, Val Loss: 0.0031\n",
            "Epoch 131/300 completed in 0.51 seconds.\n",
            "Epoch [132/300], Train Loss: 0.0050, Val Loss: 0.0016\n",
            "Epoch 132/300 completed in 0.50 seconds.\n",
            "Epoch [133/300], Train Loss: 0.0038, Val Loss: 0.0023\n",
            "Epoch 133/300 completed in 0.49 seconds.\n",
            "Epoch [134/300], Train Loss: 0.0048, Val Loss: 0.0035\n",
            "Epoch 134/300 completed in 0.50 seconds.\n",
            "Epoch [135/300], Train Loss: 0.0062, Val Loss: 0.0037\n",
            "Epoch 135/300 completed in 0.50 seconds.\n",
            "Epoch [136/300], Train Loss: 0.0053, Val Loss: 0.0023\n",
            "Epoch 136/300 completed in 0.51 seconds.\n",
            "Epoch [137/300], Train Loss: 0.0067, Val Loss: 0.0013\n",
            "Epoch 137/300 completed in 0.51 seconds.\n",
            "Epoch [138/300], Train Loss: 0.0036, Val Loss: 0.0049\n",
            "Epoch 138/300 completed in 0.50 seconds.\n",
            "Epoch [139/300], Train Loss: 0.0038, Val Loss: 0.0036\n",
            "Epoch 139/300 completed in 0.50 seconds.\n",
            "Epoch [140/300], Train Loss: 0.0041, Val Loss: 0.0018\n",
            "Epoch 140/300 completed in 0.50 seconds.\n",
            "Epoch [141/300], Train Loss: 0.0088, Val Loss: 0.0033\n",
            "Epoch 141/300 completed in 0.52 seconds.\n",
            "Epoch [142/300], Train Loss: 0.0050, Val Loss: 0.0018\n",
            "Epoch 142/300 completed in 0.52 seconds.\n",
            "Epoch [143/300], Train Loss: 0.0058, Val Loss: 0.0014\n",
            "Epoch 143/300 completed in 0.50 seconds.\n",
            "Epoch [144/300], Train Loss: 0.0046, Val Loss: 0.0026\n",
            "Epoch 144/300 completed in 0.51 seconds.\n",
            "Epoch [145/300], Train Loss: 0.0051, Val Loss: 0.0017\n",
            "Epoch 145/300 completed in 0.50 seconds.\n",
            "Epoch [146/300], Train Loss: 0.0059, Val Loss: 0.0025\n",
            "Epoch 146/300 completed in 0.50 seconds.\n",
            "Epoch [147/300], Train Loss: 0.0057, Val Loss: 0.0017\n",
            "Epoch 147/300 completed in 0.51 seconds.\n",
            "Epoch [148/300], Train Loss: 0.0047, Val Loss: 0.0030\n",
            "Epoch 148/300 completed in 0.65 seconds.\n",
            "Epoch [149/300], Train Loss: 0.0057, Val Loss: 0.0013\n",
            "Epoch 149/300 completed in 0.66 seconds.\n",
            "Epoch [150/300], Train Loss: 0.0043, Val Loss: 0.0010\n",
            "Epoch 150/300 completed in 0.74 seconds.\n",
            "Epoch [151/300], Train Loss: 0.0036, Val Loss: 0.0029\n",
            "Epoch 151/300 completed in 0.62 seconds.\n",
            "Epoch [152/300], Train Loss: 0.0074, Val Loss: 0.0063\n",
            "Epoch 152/300 completed in 0.51 seconds.\n",
            "Epoch [153/300], Train Loss: 0.0054, Val Loss: 0.0031\n",
            "Epoch 153/300 completed in 0.51 seconds.\n",
            "Epoch [154/300], Train Loss: 0.0041, Val Loss: 0.0026\n",
            "Epoch 154/300 completed in 0.50 seconds.\n",
            "Epoch [155/300], Train Loss: 0.0039, Val Loss: 0.0025\n",
            "Epoch 155/300 completed in 0.52 seconds.\n",
            "Epoch [156/300], Train Loss: 0.0033, Val Loss: 0.0018\n",
            "Epoch 156/300 completed in 0.51 seconds.\n",
            "Epoch [157/300], Train Loss: 0.0083, Val Loss: 0.0026\n",
            "Epoch 157/300 completed in 0.52 seconds.\n",
            "Epoch [158/300], Train Loss: 0.0054, Val Loss: 0.0022\n",
            "Epoch 158/300 completed in 0.51 seconds.\n",
            "Epoch [159/300], Train Loss: 0.0043, Val Loss: 0.0025\n",
            "Epoch 159/300 completed in 0.53 seconds.\n",
            "Epoch [160/300], Train Loss: 0.0041, Val Loss: 0.0019\n",
            "Epoch 160/300 completed in 0.51 seconds.\n",
            "Epoch [161/300], Train Loss: 0.0051, Val Loss: 0.0013\n",
            "Epoch 161/300 completed in 0.50 seconds.\n",
            "Epoch [162/300], Train Loss: 0.0048, Val Loss: 0.0038\n",
            "Epoch 162/300 completed in 0.51 seconds.\n",
            "Epoch [163/300], Train Loss: 0.0050, Val Loss: 0.0017\n",
            "Epoch 163/300 completed in 0.50 seconds.\n",
            "Epoch [164/300], Train Loss: 0.0054, Val Loss: 0.0020\n",
            "Epoch 164/300 completed in 0.53 seconds.\n",
            "Epoch [165/300], Train Loss: 0.0033, Val Loss: 0.0021\n",
            "Epoch 165/300 completed in 0.51 seconds.\n",
            "Epoch [166/300], Train Loss: 0.0056, Val Loss: 0.0061\n",
            "Epoch 166/300 completed in 0.52 seconds.\n",
            "Epoch [167/300], Train Loss: 0.0039, Val Loss: 0.0014\n",
            "Epoch 167/300 completed in 0.50 seconds.\n",
            "Epoch [168/300], Train Loss: 0.0044, Val Loss: 0.0029\n",
            "Epoch 168/300 completed in 0.52 seconds.\n",
            "Epoch [169/300], Train Loss: 0.0036, Val Loss: 0.0016\n",
            "Epoch 169/300 completed in 0.70 seconds.\n",
            "Epoch [170/300], Train Loss: 0.0041, Val Loss: 0.0018\n",
            "Epoch 170/300 completed in 0.68 seconds.\n",
            "Epoch [171/300], Train Loss: 0.0060, Val Loss: 0.0032\n",
            "Epoch 171/300 completed in 0.72 seconds.\n",
            "Epoch [172/300], Train Loss: 0.0052, Val Loss: 0.0028\n",
            "Epoch 172/300 completed in 0.58 seconds.\n",
            "Epoch [173/300], Train Loss: 0.0050, Val Loss: 0.0011\n",
            "Epoch 173/300 completed in 0.51 seconds.\n",
            "Epoch [174/300], Train Loss: 0.0040, Val Loss: 0.0015\n",
            "Epoch 174/300 completed in 0.51 seconds.\n",
            "Epoch [175/300], Train Loss: 0.0050, Val Loss: 0.0027\n",
            "Epoch 175/300 completed in 0.50 seconds.\n",
            "Epoch [176/300], Train Loss: 0.0043, Val Loss: 0.0044\n",
            "Epoch 176/300 completed in 0.52 seconds.\n",
            "Epoch [177/300], Train Loss: 0.0042, Val Loss: 0.0009\n",
            "Epoch 177/300 completed in 0.51 seconds.\n",
            "Epoch [178/300], Train Loss: 0.0050, Val Loss: 0.0023\n",
            "Epoch 178/300 completed in 0.49 seconds.\n",
            "Epoch [179/300], Train Loss: 0.0058, Val Loss: 0.0011\n",
            "Epoch 179/300 completed in 0.55 seconds.\n",
            "Epoch [180/300], Train Loss: 0.0052, Val Loss: 0.0028\n",
            "Epoch 180/300 completed in 0.49 seconds.\n",
            "Epoch [181/300], Train Loss: 0.0040, Val Loss: 0.0010\n",
            "Epoch 181/300 completed in 0.50 seconds.\n",
            "Epoch [182/300], Train Loss: 0.0043, Val Loss: 0.0013\n",
            "Epoch 182/300 completed in 0.50 seconds.\n",
            "Epoch [183/300], Train Loss: 0.0042, Val Loss: 0.0023\n",
            "Epoch 183/300 completed in 0.50 seconds.\n",
            "Epoch [184/300], Train Loss: 0.0038, Val Loss: 0.0015\n",
            "Epoch 184/300 completed in 0.49 seconds.\n",
            "Epoch [185/300], Train Loss: 0.0041, Val Loss: 0.0021\n",
            "Epoch 185/300 completed in 0.51 seconds.\n",
            "Epoch [186/300], Train Loss: 0.0036, Val Loss: 0.0026\n",
            "Epoch 186/300 completed in 0.51 seconds.\n",
            "Epoch [187/300], Train Loss: 0.0043, Val Loss: 0.0009\n",
            "Epoch 187/300 completed in 0.49 seconds.\n",
            "Epoch [188/300], Train Loss: 0.0040, Val Loss: 0.0113\n",
            "Epoch 188/300 completed in 0.52 seconds.\n",
            "Epoch [189/300], Train Loss: 0.0049, Val Loss: 0.0014\n",
            "Epoch 189/300 completed in 0.51 seconds.\n",
            "Epoch [190/300], Train Loss: 0.0032, Val Loss: 0.0016\n",
            "Epoch 190/300 completed in 0.69 seconds.\n",
            "Epoch [191/300], Train Loss: 0.0055, Val Loss: 0.0022\n",
            "Epoch 191/300 completed in 0.67 seconds.\n",
            "Epoch [192/300], Train Loss: 0.0039, Val Loss: 0.0018\n",
            "Epoch 192/300 completed in 0.72 seconds.\n",
            "Epoch [193/300], Train Loss: 0.0034, Val Loss: 0.0035\n",
            "Epoch 193/300 completed in 0.62 seconds.\n",
            "Epoch [194/300], Train Loss: 0.0049, Val Loss: 0.0013\n",
            "Epoch 194/300 completed in 0.50 seconds.\n",
            "Epoch [195/300], Train Loss: 0.0045, Val Loss: 0.0022\n",
            "Epoch 195/300 completed in 0.50 seconds.\n",
            "Epoch [196/300], Train Loss: 0.0036, Val Loss: 0.0012\n",
            "Epoch 196/300 completed in 0.51 seconds.\n",
            "Epoch [197/300], Train Loss: 0.0030, Val Loss: 0.0014\n",
            "Epoch 197/300 completed in 0.51 seconds.\n",
            "Epoch [198/300], Train Loss: 0.0036, Val Loss: 0.0016\n",
            "Epoch 198/300 completed in 0.51 seconds.\n",
            "Epoch [199/300], Train Loss: 0.0041, Val Loss: 0.0030\n",
            "Epoch 199/300 completed in 0.50 seconds.\n",
            "Epoch [200/300], Train Loss: 0.0047, Val Loss: 0.0021\n",
            "Epoch 200/300 completed in 0.51 seconds.\n",
            "Epoch [201/300], Train Loss: 0.0031, Val Loss: 0.0015\n",
            "Epoch 201/300 completed in 0.49 seconds.\n",
            "Epoch [202/300], Train Loss: 0.0032, Val Loss: 0.0021\n",
            "Epoch 202/300 completed in 0.52 seconds.\n",
            "Epoch [203/300], Train Loss: 0.0047, Val Loss: 0.0039\n",
            "Epoch 203/300 completed in 0.49 seconds.\n",
            "Epoch [204/300], Train Loss: 0.0039, Val Loss: 0.0016\n",
            "Epoch 204/300 completed in 0.50 seconds.\n",
            "Epoch [205/300], Train Loss: 0.0055, Val Loss: 0.0016\n",
            "Epoch 205/300 completed in 0.51 seconds.\n",
            "Epoch [206/300], Train Loss: 0.0044, Val Loss: 0.0014\n",
            "Epoch 206/300 completed in 0.52 seconds.\n",
            "Epoch [207/300], Train Loss: 0.0029, Val Loss: 0.0062\n",
            "Epoch 207/300 completed in 0.51 seconds.\n",
            "Epoch [208/300], Train Loss: 0.0034, Val Loss: 0.0012\n",
            "Epoch 208/300 completed in 0.50 seconds.\n",
            "Epoch [209/300], Train Loss: 0.0036, Val Loss: 0.0055\n",
            "Epoch 209/300 completed in 0.51 seconds.\n",
            "Epoch [210/300], Train Loss: 0.0042, Val Loss: 0.0013\n",
            "Epoch 210/300 completed in 0.50 seconds.\n",
            "Epoch [211/300], Train Loss: 0.0046, Val Loss: 0.0018\n",
            "Epoch 211/300 completed in 0.66 seconds.\n",
            "Epoch [212/300], Train Loss: 0.0036, Val Loss: 0.0040\n",
            "Epoch 212/300 completed in 0.65 seconds.\n",
            "Epoch [213/300], Train Loss: 0.0039, Val Loss: 0.0034\n",
            "Epoch 213/300 completed in 0.71 seconds.\n",
            "Epoch [214/300], Train Loss: 0.0039, Val Loss: 0.0011\n",
            "Epoch 214/300 completed in 0.66 seconds.\n",
            "Epoch [215/300], Train Loss: 0.0031, Val Loss: 0.0025\n",
            "Epoch 215/300 completed in 0.51 seconds.\n",
            "Epoch [216/300], Train Loss: 0.0037, Val Loss: 0.0023\n",
            "Epoch 216/300 completed in 0.50 seconds.\n",
            "Epoch [217/300], Train Loss: 0.0038, Val Loss: 0.0029\n",
            "Epoch 217/300 completed in 0.52 seconds.\n",
            "Epoch [218/300], Train Loss: 0.0041, Val Loss: 0.0044\n",
            "Epoch 218/300 completed in 0.50 seconds.\n",
            "Epoch [219/300], Train Loss: 0.0050, Val Loss: 0.0025\n",
            "Epoch 219/300 completed in 0.52 seconds.\n",
            "Epoch [220/300], Train Loss: 0.0030, Val Loss: 0.0013\n",
            "Epoch 220/300 completed in 0.49 seconds.\n",
            "Epoch [221/300], Train Loss: 0.0035, Val Loss: 0.0024\n",
            "Epoch 221/300 completed in 0.51 seconds.\n",
            "Epoch [222/300], Train Loss: 0.0032, Val Loss: 0.0010\n",
            "Epoch 222/300 completed in 0.50 seconds.\n",
            "Epoch [223/300], Train Loss: 0.0042, Val Loss: 0.0038\n",
            "Epoch 223/300 completed in 0.49 seconds.\n",
            "Epoch [224/300], Train Loss: 0.0039, Val Loss: 0.0023\n",
            "Epoch 224/300 completed in 0.51 seconds.\n",
            "Epoch [225/300], Train Loss: 0.0036, Val Loss: 0.0018\n",
            "Epoch 225/300 completed in 0.50 seconds.\n",
            "Epoch [226/300], Train Loss: 0.0029, Val Loss: 0.0013\n",
            "Epoch 226/300 completed in 0.51 seconds.\n",
            "Epoch [227/300], Train Loss: 0.0029, Val Loss: 0.0033\n",
            "Epoch 227/300 completed in 0.50 seconds.\n",
            "Epoch [228/300], Train Loss: 0.0049, Val Loss: 0.0016\n",
            "Epoch 228/300 completed in 0.53 seconds.\n",
            "Epoch [229/300], Train Loss: 0.0028, Val Loss: 0.0029\n",
            "Epoch 229/300 completed in 0.50 seconds.\n",
            "Epoch [230/300], Train Loss: 0.0050, Val Loss: 0.0017\n",
            "Epoch 230/300 completed in 0.55 seconds.\n",
            "Epoch [231/300], Train Loss: 0.0037, Val Loss: 0.0021\n",
            "Epoch 231/300 completed in 0.50 seconds.\n",
            "Epoch [232/300], Train Loss: 0.0029, Val Loss: 0.0038\n",
            "Epoch 232/300 completed in 0.65 seconds.\n",
            "Epoch [233/300], Train Loss: 0.0051, Val Loss: 0.0061\n",
            "Epoch 233/300 completed in 0.65 seconds.\n",
            "Epoch [234/300], Train Loss: 0.0038, Val Loss: 0.0020\n",
            "Epoch 234/300 completed in 0.73 seconds.\n",
            "Epoch [235/300], Train Loss: 0.0030, Val Loss: 0.0010\n",
            "Epoch 235/300 completed in 0.65 seconds.\n",
            "Epoch [236/300], Train Loss: 0.0030, Val Loss: 0.0017\n",
            "Epoch 236/300 completed in 0.50 seconds.\n",
            "Epoch [237/300], Train Loss: 0.0036, Val Loss: 0.0072\n",
            "Epoch 237/300 completed in 0.50 seconds.\n",
            "Epoch [238/300], Train Loss: 0.0058, Val Loss: 0.0041\n",
            "Epoch 238/300 completed in 0.49 seconds.\n",
            "Epoch [239/300], Train Loss: 0.0030, Val Loss: 0.0020\n",
            "Epoch 239/300 completed in 0.49 seconds.\n",
            "Epoch [240/300], Train Loss: 0.0039, Val Loss: 0.0021\n",
            "Epoch 240/300 completed in 0.51 seconds.\n",
            "Epoch [241/300], Train Loss: 0.0033, Val Loss: 0.0013\n",
            "Epoch 241/300 completed in 0.51 seconds.\n",
            "Epoch [242/300], Train Loss: 0.0024, Val Loss: 0.0021\n",
            "Epoch 242/300 completed in 0.50 seconds.\n",
            "Epoch [243/300], Train Loss: 0.0030, Val Loss: 0.0018\n",
            "Epoch 243/300 completed in 0.51 seconds.\n",
            "Epoch [244/300], Train Loss: 0.0035, Val Loss: 0.0022\n",
            "Epoch 244/300 completed in 0.50 seconds.\n",
            "Epoch [245/300], Train Loss: 0.0035, Val Loss: 0.0012\n",
            "Epoch 245/300 completed in 0.50 seconds.\n",
            "Epoch [246/300], Train Loss: 0.0025, Val Loss: 0.0009\n",
            "Epoch 246/300 completed in 0.49 seconds.\n",
            "Epoch [247/300], Train Loss: 0.0032, Val Loss: 0.0032\n",
            "Epoch 247/300 completed in 0.50 seconds.\n",
            "Epoch [248/300], Train Loss: 0.0031, Val Loss: 0.0017\n",
            "Epoch 248/300 completed in 0.51 seconds.\n",
            "Epoch [249/300], Train Loss: 0.0029, Val Loss: 0.0017\n",
            "Epoch 249/300 completed in 0.51 seconds.\n",
            "Epoch [250/300], Train Loss: 0.0034, Val Loss: 0.0025\n",
            "Epoch 250/300 completed in 0.50 seconds.\n",
            "Epoch [251/300], Train Loss: 0.0038, Val Loss: 0.0023\n",
            "Epoch 251/300 completed in 0.51 seconds.\n",
            "Epoch [252/300], Train Loss: 0.0049, Val Loss: 0.0009\n",
            "Epoch 252/300 completed in 0.51 seconds.\n",
            "Epoch [253/300], Train Loss: 0.0029, Val Loss: 0.0012\n",
            "Epoch 253/300 completed in 0.59 seconds.\n",
            "Epoch [254/300], Train Loss: 0.0033, Val Loss: 0.0024\n",
            "Epoch 254/300 completed in 0.66 seconds.\n",
            "Epoch [255/300], Train Loss: 0.0033, Val Loss: 0.0033\n",
            "Epoch 255/300 completed in 0.72 seconds.\n",
            "Epoch [256/300], Train Loss: 0.0048, Val Loss: 0.0021\n",
            "Epoch 256/300 completed in 0.73 seconds.\n",
            "Epoch [257/300], Train Loss: 0.0034, Val Loss: 0.0025\n",
            "Epoch 257/300 completed in 0.53 seconds.\n",
            "Epoch [258/300], Train Loss: 0.0028, Val Loss: 0.0012\n",
            "Epoch 258/300 completed in 0.50 seconds.\n",
            "Epoch [259/300], Train Loss: 0.0026, Val Loss: 0.0011\n",
            "Epoch 259/300 completed in 0.50 seconds.\n",
            "Epoch [260/300], Train Loss: 0.0028, Val Loss: 0.0014\n",
            "Epoch 260/300 completed in 0.65 seconds.\n",
            "Epoch [261/300], Train Loss: 0.0034, Val Loss: 0.0023\n",
            "Epoch 261/300 completed in 0.64 seconds.\n",
            "Epoch [262/300], Train Loss: 0.0029, Val Loss: 0.0026\n",
            "Epoch 262/300 completed in 0.77 seconds.\n",
            "Epoch [263/300], Train Loss: 0.0038, Val Loss: 0.0073\n",
            "Epoch 263/300 completed in 0.73 seconds.\n",
            "Epoch [264/300], Train Loss: 0.0031, Val Loss: 0.0025\n",
            "Epoch 264/300 completed in 0.51 seconds.\n",
            "Epoch [265/300], Train Loss: 0.0037, Val Loss: 0.0042\n",
            "Epoch 265/300 completed in 0.50 seconds.\n",
            "Epoch [266/300], Train Loss: 0.0033, Val Loss: 0.0026\n",
            "Epoch 266/300 completed in 0.51 seconds.\n",
            "Epoch [267/300], Train Loss: 0.0027, Val Loss: 0.0017\n",
            "Epoch 267/300 completed in 0.50 seconds.\n",
            "Epoch [268/300], Train Loss: 0.0031, Val Loss: 0.0017\n",
            "Epoch 268/300 completed in 0.52 seconds.\n",
            "Epoch [269/300], Train Loss: 0.0029, Val Loss: 0.0029\n",
            "Epoch 269/300 completed in 0.51 seconds.\n",
            "Epoch [270/300], Train Loss: 0.0033, Val Loss: 0.0057\n",
            "Epoch 270/300 completed in 0.52 seconds.\n",
            "Epoch [271/300], Train Loss: 0.0034, Val Loss: 0.0011\n",
            "Epoch 271/300 completed in 0.50 seconds.\n",
            "Epoch [272/300], Train Loss: 0.0030, Val Loss: 0.0023\n",
            "Epoch 272/300 completed in 0.51 seconds.\n",
            "Epoch [273/300], Train Loss: 0.0030, Val Loss: 0.0019\n",
            "Epoch 273/300 completed in 0.63 seconds.\n",
            "Epoch [274/300], Train Loss: 0.0024, Val Loss: 0.0029\n",
            "Epoch 274/300 completed in 0.63 seconds.\n",
            "Epoch [275/300], Train Loss: 0.0032, Val Loss: 0.0035\n",
            "Epoch 275/300 completed in 0.74 seconds.\n",
            "Epoch [276/300], Train Loss: 0.0034, Val Loss: 0.0027\n",
            "Epoch 276/300 completed in 0.72 seconds.\n",
            "Epoch [277/300], Train Loss: 0.0037, Val Loss: 0.0023\n",
            "Epoch 277/300 completed in 0.50 seconds.\n",
            "Epoch [278/300], Train Loss: 0.0026, Val Loss: 0.0013\n",
            "Epoch 278/300 completed in 0.53 seconds.\n",
            "Epoch [279/300], Train Loss: 0.0027, Val Loss: 0.0027\n",
            "Epoch 279/300 completed in 0.50 seconds.\n",
            "Epoch [280/300], Train Loss: 0.0036, Val Loss: 0.0026\n",
            "Epoch 280/300 completed in 0.51 seconds.\n",
            "Epoch [281/300], Train Loss: 0.0029, Val Loss: 0.0022\n",
            "Epoch 281/300 completed in 0.51 seconds.\n",
            "Epoch [282/300], Train Loss: 0.0024, Val Loss: 0.0020\n",
            "Epoch 282/300 completed in 0.49 seconds.\n",
            "Epoch [283/300], Train Loss: 0.0035, Val Loss: 0.0010\n",
            "Epoch 283/300 completed in 0.52 seconds.\n",
            "Epoch [284/300], Train Loss: 0.0038, Val Loss: 0.0044\n",
            "Epoch 284/300 completed in 0.51 seconds.\n",
            "Epoch [285/300], Train Loss: 0.0031, Val Loss: 0.0011\n",
            "Epoch 285/300 completed in 0.52 seconds.\n",
            "Epoch [286/300], Train Loss: 0.0029, Val Loss: 0.0018\n",
            "Epoch 286/300 completed in 0.50 seconds.\n",
            "Epoch [287/300], Train Loss: 0.0036, Val Loss: 0.0046\n",
            "Epoch 287/300 completed in 0.51 seconds.\n",
            "Epoch [288/300], Train Loss: 0.0033, Val Loss: 0.0013\n",
            "Epoch 288/300 completed in 0.51 seconds.\n",
            "Epoch [289/300], Train Loss: 0.0027, Val Loss: 0.0022\n",
            "Epoch 289/300 completed in 0.51 seconds.\n",
            "Epoch [290/300], Train Loss: 0.0030, Val Loss: 0.0011\n",
            "Epoch 290/300 completed in 0.50 seconds.\n",
            "Epoch [291/300], Train Loss: 0.0030, Val Loss: 0.0046\n",
            "Epoch 291/300 completed in 0.50 seconds.\n",
            "Epoch [292/300], Train Loss: 0.0035, Val Loss: 0.0047\n",
            "Epoch 292/300 completed in 0.50 seconds.\n",
            "Epoch [293/300], Train Loss: 0.0032, Val Loss: 0.0031\n",
            "Epoch 293/300 completed in 0.50 seconds.\n",
            "Epoch [294/300], Train Loss: 0.0024, Val Loss: 0.0029\n",
            "Epoch 294/300 completed in 0.56 seconds.\n",
            "Epoch [295/300], Train Loss: 0.0029, Val Loss: 0.0025\n",
            "Epoch 295/300 completed in 0.64 seconds.\n",
            "Epoch [296/300], Train Loss: 0.0027, Val Loss: 0.0012\n",
            "Epoch 296/300 completed in 0.71 seconds.\n",
            "Epoch [297/300], Train Loss: 0.0024, Val Loss: 0.0029\n",
            "Epoch 297/300 completed in 0.74 seconds.\n",
            "Epoch [298/300], Train Loss: 0.0028, Val Loss: 0.0029\n",
            "Epoch 298/300 completed in 0.49 seconds.\n",
            "Epoch [299/300], Train Loss: 0.0033, Val Loss: 0.0018\n",
            "Epoch 299/300 completed in 0.50 seconds.\n",
            "Epoch [300/300], Train Loss: 0.0025, Val Loss: 0.0028\n",
            "Epoch 300/300 completed in 0.50 seconds.\n",
            "Total training time: 162.75 seconds.\n",
            "Model saved: best_model_with_losses.pth\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "best_model.train()\n",
        "total_train_time = 0  # 총 학습 시간 기록 변수\n",
        "\n",
        "train_losses = [] # 손실 저장할 리스트\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):  # 최종 모델 학습\n",
        "    best_model.train()\n",
        "    train_loss = 0.0\n",
        "    epoch_start_time = time.time()  # 에포크 시작 시간 기록\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x = batch_x.to(device)  # 입력 데이터 차원 조정\n",
        "        batch_y = batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = best_model(batch_x)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    epoch_end_time = time.time()  # 에포크 종료 시간 기록\n",
        "    epoch_duration = epoch_end_time - epoch_start_time  # 한 에포크의 학습 시간\n",
        "    total_train_time += epoch_duration  # 총 학습 시간에 추가\n",
        "\n",
        "    # Validation set에서 성능 측정\n",
        "    best_model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # 입력 데이터 차원 조정\n",
        "            output = best_model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # 평균 손실 계산 및 저장\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "    val_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # 모델 저장\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "\n",
        "      # Model Save\n",
        "      model_save_path = f\"best_model_epoch_{epoch+1}.pth\"\n",
        "      torch.save({\n",
        "        'model_state_dict': best_model.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses\n",
        "      }, model_save_path)\n",
        "      print(f\"Model saved: {model_save_path}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} completed in {epoch_duration:.2f} seconds.\")\n",
        "\n",
        "print(f\"Total training time: {total_train_time:.2f} seconds.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}